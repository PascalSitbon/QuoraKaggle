{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm, preprocessing, cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.learning_curve import learning_curve\n",
    "import random\n",
    "import numpy as np\n",
    "import gensim\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import nltk\n",
    "import itertools\n",
    "import glove\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize, ngrams\n",
    "import re\n",
    "from gensim import corpora\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pascalsitbon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Loading Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training_Set = pd.read_csv('train.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404288, 6)\n"
     ]
    }
   ],
   "source": [
    "print(Training_Set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method to find separation of slits using fresnel biprism?\n"
     ]
    }
   ],
   "source": [
    "print(Training_Set.iloc[10,:]['question1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = Training_Set[['question1','question2','is_duplicate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_train = clean_data[:,:2]\n",
    "labels = clean_data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of data \n",
      " ['What is the step by step guide to invest in share market in india?'\n",
      " 'What is the step by step guide to invest in share market?'] 0\n",
      "sample of data \n",
      " ['What is the story of Kohinoor (Koh-i-Noor) Diamond?'\n",
      " 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?'] 0\n",
      "sample of data \n",
      " [ 'How can I increase the speed of my internet connection while using a VPN?'\n",
      " 'How can Internet speed be increased by hacking through DNS?'] 0\n",
      "sample of data \n",
      " ['Why am I mentally very lonely? How can I solve it?'\n",
      " 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?'] 0\n",
      "sample of data \n",
      " [ 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?'\n",
      " 'Which fish would survive in salt water?'] 0\n",
      "sample of data \n",
      " [ 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?'\n",
      " \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"] 1\n",
      "sample of data \n",
      " ['Should I buy tiago?'\n",
      " 'What keeps childern active and far from phone and video games?'] 0\n",
      "sample of data \n",
      " ['How can I be a good geologist?'\n",
      " 'What should I do to be a great geologist?'] 1\n",
      "sample of data \n",
      " ['When do you use シ instead of し?' 'When do you use \"&\" instead of \"and\"?'] 0\n",
      "sample of data \n",
      " ['Motorola (company): Can I hack my Charter Motorolla DCX3400?'\n",
      " 'How do I hack Motorola DCX3400 for free internet?'] 0\n",
      "sample of data \n",
      " ['Method to find separation of slits using fresnel biprism?'\n",
      " 'What are some of the things technicians can tell about the durability and reliability of Laptops and its components?'] 0\n",
      "sample of data \n",
      " ['How do I read and find my YouTube comments?'\n",
      " 'How can I see all my Youtube comments?'] 1\n",
      "sample of data \n",
      " ['What can make Physics easy to learn?'\n",
      " 'How can you make physics easy to learn?'] 1\n",
      "sample of data \n",
      " ['What was your first sexual experience like?'\n",
      " 'What was your first sexual experience?'] 1\n",
      "sample of data \n",
      " [ 'What are the laws to change your status from a student visa to a green card in the US, how do they compare to the immigration laws in Canada?'\n",
      " 'What are the laws to change your status from a student visa to a green card in the US? How do they compare to the immigration laws in Japan?'] 0\n",
      "sample of data \n",
      " [ 'What would a Trump presidency mean for current international master’s students on an F1 visa?'\n",
      " 'How will a Trump presidency affect the students presently in US or planning to study in US?'] 1\n",
      "sample of data \n",
      " ['What does manipulation mean?' 'What does manipulation means?'] 1\n",
      "sample of data \n",
      " ['Why do girls want to be friends with the guy they reject?'\n",
      " 'How do guys feel after rejecting a girl?'] 0\n",
      "sample of data \n",
      " [ 'Why are so many Quora users posting questions that are readily answered on Google?'\n",
      " 'Why do people ask Quora questions which can be answered easily by Google?'] 1\n",
      "sample of data \n",
      " ['Which is the best digital marketing institution in banglore?'\n",
      " 'Which is the best digital marketing institute in Pune?'] 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('sample of data','\\n',sentences_train[i,:],labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_train = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence_train):\n",
    "    sentences = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(sentence_train.shape[0]): \n",
    "        source_sentence = sentences_train[i,0].lower().split(\" \") \n",
    "        source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "        unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "        sentences.append(unigrams_que1)\n",
    "\n",
    "        target_sentence = sentence_train[i,1].lower().split(\" \") \n",
    "        target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "        unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "        sentences.append(unigrams_que2)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = preprocess(sentences_train[:size_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:28:23,282 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-05-16 15:28:23,283 : INFO : collecting all words and their counts\n",
      "2017-05-16 15:28:23,284 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-16 15:28:23,304 : INFO : PROGRESS: at sentence #10000, processed 57070 words, keeping 13175 word types\n",
      "2017-05-16 15:28:23,323 : INFO : PROGRESS: at sentence #20000, processed 113800 words, keeping 20410 word types\n",
      "2017-05-16 15:28:23,345 : INFO : PROGRESS: at sentence #30000, processed 170684 words, keeping 26246 word types\n",
      "2017-05-16 15:28:23,370 : INFO : PROGRESS: at sentence #40000, processed 227355 words, keeping 31320 word types\n",
      "2017-05-16 15:28:23,390 : INFO : PROGRESS: at sentence #50000, processed 284506 words, keeping 36058 word types\n",
      "2017-05-16 15:28:23,410 : INFO : PROGRESS: at sentence #60000, processed 341143 words, keeping 40245 word types\n",
      "2017-05-16 15:28:23,433 : INFO : PROGRESS: at sentence #70000, processed 397752 words, keeping 44200 word types\n",
      "2017-05-16 15:28:23,455 : INFO : PROGRESS: at sentence #80000, processed 454391 words, keeping 47821 word types\n",
      "2017-05-16 15:28:23,476 : INFO : PROGRESS: at sentence #90000, processed 511330 words, keeping 51264 word types\n",
      "2017-05-16 15:28:23,498 : INFO : PROGRESS: at sentence #100000, processed 568451 words, keeping 54700 word types\n",
      "2017-05-16 15:28:23,519 : INFO : PROGRESS: at sentence #110000, processed 625141 words, keeping 57878 word types\n",
      "2017-05-16 15:28:23,542 : INFO : PROGRESS: at sentence #120000, processed 682074 words, keeping 60984 word types\n",
      "2017-05-16 15:28:23,568 : INFO : PROGRESS: at sentence #130000, processed 738507 words, keeping 63887 word types\n",
      "2017-05-16 15:28:23,590 : INFO : PROGRESS: at sentence #140000, processed 795857 words, keeping 66813 word types\n",
      "2017-05-16 15:28:23,617 : INFO : PROGRESS: at sentence #150000, processed 852298 words, keeping 69468 word types\n",
      "2017-05-16 15:28:23,646 : INFO : PROGRESS: at sentence #160000, processed 909999 words, keeping 72112 word types\n",
      "2017-05-16 15:28:23,681 : INFO : PROGRESS: at sentence #170000, processed 966538 words, keeping 74630 word types\n",
      "2017-05-16 15:28:23,723 : INFO : PROGRESS: at sentence #180000, processed 1023360 words, keeping 77124 word types\n",
      "2017-05-16 15:28:23,748 : INFO : PROGRESS: at sentence #190000, processed 1080862 words, keeping 79710 word types\n",
      "2017-05-16 15:28:23,772 : INFO : collected 82063 word types from a corpus of 1136863 raw words and 200000 sentences\n",
      "2017-05-16 15:28:23,773 : INFO : Loading a fresh vocabulary\n",
      "2017-05-16 15:28:24,163 : INFO : min_count=1 retains 82063 unique words (100% of original 82063, drops 0)\n",
      "2017-05-16 15:28:24,164 : INFO : min_count=1 leaves 1136863 word corpus (100% of original 1136863, drops 0)\n",
      "2017-05-16 15:28:24,423 : INFO : deleting the raw counts dictionary of 82063 items\n",
      "2017-05-16 15:28:24,427 : INFO : sample=0.001 downsamples 16 most-common words\n",
      "2017-05-16 15:28:24,428 : INFO : downsampling leaves estimated 1098819 word corpus (96.7% of prior 1136863)\n",
      "2017-05-16 15:28:24,428 : INFO : estimated required memory for 82063 words and 50 dimensions: 73856700 bytes\n",
      "2017-05-16 15:28:24,835 : INFO : resetting layer weights\n",
      "2017-05-16 15:28:26,554 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-16 15:28:27,582 : INFO : PROGRESS: at 10.37% examples, 562365 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:28:28,587 : INFO : PROGRESS: at 21.28% examples, 579394 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:29,588 : INFO : PROGRESS: at 31.13% examples, 566427 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:28:30,597 : INFO : PROGRESS: at 43.96% examples, 599709 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:31,610 : INFO : PROGRESS: at 56.98% examples, 620958 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:28:32,641 : INFO : PROGRESS: at 67.36% examples, 609451 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:33,646 : INFO : PROGRESS: at 78.60% examples, 610245 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:34,667 : INFO : PROGRESS: at 91.28% examples, 619261 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:35,561 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:28:35,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:28:35,572 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:28:35,581 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:28:35,582 : INFO : training on 5684315 raw words (5493975 effective words) took 9.0s, 609552 effective words/s\n",
      "2017-05-16 15:28:35,635 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:28:36,660 : INFO : PROGRESS: at 11.26% examples, 612709 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:37,667 : INFO : PROGRESS: at 21.98% examples, 598980 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:28:38,688 : INFO : PROGRESS: at 33.77% examples, 610770 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:39,691 : INFO : PROGRESS: at 45.19% examples, 614598 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:40,697 : INFO : PROGRESS: at 56.62% examples, 616601 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 15:28:41,707 : INFO : PROGRESS: at 68.59% examples, 622156 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:42,719 : INFO : PROGRESS: at 80.54% examples, 626077 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:43,721 : INFO : PROGRESS: at 92.16% examples, 627386 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:44,326 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:28:44,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:28:44,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:28:44,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:28:44,353 : INFO : training on 5684315 raw words (5494191 effective words) took 8.7s, 631355 effective words/s\n",
      "2017-05-16 15:28:44,353 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:28:45,401 : INFO : PROGRESS: at 12.14% examples, 651858 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:46,406 : INFO : PROGRESS: at 24.45% examples, 662282 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:47,421 : INFO : PROGRESS: at 37.28% examples, 673340 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:48,436 : INFO : PROGRESS: at 49.77% examples, 673933 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:49,454 : INFO : PROGRESS: at 62.43% examples, 675746 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:28:50,474 : INFO : PROGRESS: at 74.39% examples, 670510 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:28:51,481 : INFO : PROGRESS: at 86.00% examples, 665225 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:52,561 : INFO : PROGRESS: at 94.09% examples, 631740 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:53,044 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:28:53,058 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:28:53,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:28:53,101 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:28:53,102 : INFO : training on 5684315 raw words (5493603 effective words) took 8.7s, 629738 effective words/s\n",
      "2017-05-16 15:28:53,103 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:28:54,192 : INFO : PROGRESS: at 7.57% examples, 389309 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:55,204 : INFO : PROGRESS: at 17.75% examples, 469223 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:56,219 : INFO : PROGRESS: at 25.32% examples, 449601 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:57,223 : INFO : PROGRESS: at 37.45% examples, 502145 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:28:58,226 : INFO : PROGRESS: at 49.60% examples, 534108 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:28:59,231 : INFO : PROGRESS: at 62.26% examples, 560081 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:00,246 : INFO : PROGRESS: at 74.92% examples, 578000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:01,265 : INFO : PROGRESS: at 87.76% examples, 592257 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:02,206 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:02,227 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:02,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:02,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:02,237 : INFO : training on 5684315 raw words (5493897 effective words) took 9.1s, 602892 effective words/s\n",
      "2017-05-16 15:29:02,238 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:03,255 : INFO : PROGRESS: at 12.15% examples, 666534 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:04,260 : INFO : PROGRESS: at 24.10% examples, 660009 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:29:05,262 : INFO : PROGRESS: at 35.87% examples, 655374 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:06,263 : INFO : PROGRESS: at 48.37% examples, 662803 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:07,269 : INFO : PROGRESS: at 60.85% examples, 666661 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:08,299 : INFO : PROGRESS: at 73.51% examples, 668163 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:09,316 : INFO : PROGRESS: at 86.35% examples, 671803 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:29:10,327 : INFO : PROGRESS: at 98.83% examples, 672699 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:10,399 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:10,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:10,411 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:10,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:10,433 : INFO : training on 5684315 raw words (5494070 effective words) took 8.2s, 671788 effective words/s\n",
      "2017-05-16 15:29:10,434 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:11,449 : INFO : PROGRESS: at 11.79% examples, 646619 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:12,454 : INFO : PROGRESS: at 23.74% examples, 650011 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:13,456 : INFO : PROGRESS: at 36.05% examples, 658462 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:14,488 : INFO : PROGRESS: at 48.02% examples, 652868 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:15,489 : INFO : PROGRESS: at 60.67% examples, 661295 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:16,496 : INFO : PROGRESS: at 73.69% examples, 669378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:17,498 : INFO : PROGRESS: at 86.35% examples, 672927 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:18,485 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:18,505 : INFO : PROGRESS: at 99.71% examples, 679919 words/s, in_qsize 2, out_qsize 1\n",
      "2017-05-16 15:29:18,506 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:18,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:18,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:18,510 : INFO : training on 5684315 raw words (5494052 effective words) took 8.1s, 681392 effective words/s\n",
      "2017-05-16 15:29:18,511 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:19,530 : INFO : PROGRESS: at 12.32% examples, 674848 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:20,534 : INFO : PROGRESS: at 24.79% examples, 679243 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:21,535 : INFO : PROGRESS: at 36.93% examples, 674849 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:22,546 : INFO : PROGRESS: at 49.42% examples, 675785 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:23,549 : INFO : PROGRESS: at 61.90% examples, 677433 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:24,556 : INFO : PROGRESS: at 74.04% examples, 674891 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:25,562 : INFO : PROGRESS: at 86.88% examples, 678588 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:26,552 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:26,563 : INFO : PROGRESS: at 99.71% examples, 681839 words/s, in_qsize 2, out_qsize 1\n",
      "2017-05-16 15:29:26,564 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:26,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:26,578 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:26,580 : INFO : training on 5684315 raw words (5494353 effective words) took 8.1s, 682404 effective words/s\n",
      "2017-05-16 15:29:26,581 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:27,607 : INFO : PROGRESS: at 12.50% examples, 679522 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:28,632 : INFO : PROGRESS: at 25.14% examples, 678953 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:29,646 : INFO : PROGRESS: at 38.16% examples, 687739 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:30,669 : INFO : PROGRESS: at 50.46% examples, 680977 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:31,671 : INFO : PROGRESS: at 62.25% examples, 674124 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:32,690 : INFO : PROGRESS: at 74.57% examples, 672421 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:29:33,744 : INFO : PROGRESS: at 87.06% examples, 669269 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:29:34,738 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:34,751 : INFO : PROGRESS: at 99.71% examples, 671906 words/s, in_qsize 1, out_qsize 3\n",
      "2017-05-16 15:29:34,752 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:34,752 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:34,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:34,754 : INFO : training on 5684315 raw words (5494158 effective words) took 8.2s, 673560 effective words/s\n",
      "2017-05-16 15:29:34,755 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:35,799 : INFO : PROGRESS: at 12.32% examples, 658136 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:36,818 : INFO : PROGRESS: at 25.15% examples, 675004 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:29:37,837 : INFO : PROGRESS: at 38.16% examples, 683994 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:29:38,854 : INFO : PROGRESS: at 51.00% examples, 686280 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:39,857 : INFO : PROGRESS: at 63.48% examples, 685911 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:29:40,866 : INFO : PROGRESS: at 76.14% examples, 686441 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:41,887 : INFO : PROGRESS: at 88.81% examples, 685756 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:42,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:42,821 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:42,823 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:42,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:42,824 : INFO : training on 5684315 raw words (5493828 effective words) took 8.1s, 682271 effective words/s\n",
      "2017-05-16 15:29:42,825 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:43,842 : INFO : PROGRESS: at 11.44% examples, 626927 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:44,846 : INFO : PROGRESS: at 24.27% examples, 664619 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:45,859 : INFO : PROGRESS: at 36.76% examples, 668949 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:46,863 : INFO : PROGRESS: at 48.89% examples, 667751 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:47,864 : INFO : PROGRESS: at 61.73% examples, 675032 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:48,883 : INFO : PROGRESS: at 74.39% examples, 676389 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:49,898 : INFO : PROGRESS: at 86.88% examples, 676302 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:50,892 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:50,902 : INFO : PROGRESS: at 99.71% examples, 679575 words/s, in_qsize 2, out_qsize 1\n",
      "2017-05-16 15:29:50,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:50,915 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:50,917 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:50,918 : INFO : training on 5684315 raw words (5493854 effective words) took 8.1s, 680109 effective words/s\n",
      "2017-05-16 15:29:50,919 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:51,967 : INFO : PROGRESS: at 12.14% examples, 650821 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:52,995 : INFO : PROGRESS: at 24.80% examples, 663856 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:54,012 : INFO : PROGRESS: at 36.40% examples, 651575 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:55,028 : INFO : PROGRESS: at 49.25% examples, 662193 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:29:56,036 : INFO : PROGRESS: at 62.26% examples, 671537 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:57,038 : INFO : PROGRESS: at 75.28% examples, 678476 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:29:58,041 : INFO : PROGRESS: at 87.94% examples, 680582 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:29:58,938 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:29:58,941 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:29:58,952 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:29:58,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:29:58,957 : INFO : training on 5684315 raw words (5494314 effective words) took 8.0s, 685489 effective words/s\n",
      "2017-05-16 15:29:58,958 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:29:59,998 : INFO : PROGRESS: at 12.32% examples, 660465 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:01,000 : INFO : PROGRESS: at 25.14% examples, 682002 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:02,015 : INFO : PROGRESS: at 37.63% examples, 679882 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:03,015 : INFO : PROGRESS: at 50.30% examples, 683814 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:04,045 : INFO : PROGRESS: at 62.25% examples, 674502 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:30:05,071 : INFO : PROGRESS: at 75.28% examples, 678281 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:06,090 : INFO : PROGRESS: at 88.29% examples, 681694 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:06,939 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:30:06,949 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:30:06,954 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:30:06,955 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:30:06,956 : INFO : training on 5684315 raw words (5494503 effective words) took 8.0s, 688372 effective words/s\n",
      "2017-05-16 15:30:06,957 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:30:07,978 : INFO : PROGRESS: at 12.14% examples, 666926 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:08,993 : INFO : PROGRESS: at 24.80% examples, 676182 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:09,999 : INFO : PROGRESS: at 37.45% examples, 681396 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:30:11,005 : INFO : PROGRESS: at 50.30% examples, 686292 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:12,018 : INFO : PROGRESS: at 62.60% examples, 682548 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:13,026 : INFO : PROGRESS: at 75.10% examples, 682287 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:14,028 : INFO : PROGRESS: at 86.88% examples, 677104 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:15,029 : INFO : PROGRESS: at 99.36% examples, 678059 words/s, in_qsize 3, out_qsize 2\n",
      "2017-05-16 15:30:15,037 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:30:15,038 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:30:15,052 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:30:15,053 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:30:15,054 : INFO : training on 5684315 raw words (5494009 effective words) took 8.1s, 680346 effective words/s\n",
      "2017-05-16 15:30:15,054 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:30:16,072 : INFO : PROGRESS: at 11.96% examples, 655007 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:30:17,074 : INFO : PROGRESS: at 24.97% examples, 684200 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:18,114 : INFO : PROGRESS: at 37.98% examples, 685465 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:19,120 : INFO : PROGRESS: at 50.82% examples, 689326 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:20,122 : INFO : PROGRESS: at 63.48% examples, 690256 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:30:21,128 : INFO : PROGRESS: at 76.32% examples, 692106 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:22,154 : INFO : PROGRESS: at 88.99% examples, 690093 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:22,985 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:30:22,994 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:30:23,007 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:30:23,008 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:30:23,009 : INFO : training on 5684315 raw words (5493910 effective words) took 7.9s, 691970 effective words/s\n",
      "2017-05-16 15:30:23,009 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:30:24,029 : INFO : PROGRESS: at 11.61% examples, 635038 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:25,047 : INFO : PROGRESS: at 24.62% examples, 668992 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:26,071 : INFO : PROGRESS: at 37.10% examples, 669467 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:27,073 : INFO : PROGRESS: at 50.12% examples, 680323 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:28,089 : INFO : PROGRESS: at 62.60% examples, 679300 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:29,089 : INFO : PROGRESS: at 75.28% examples, 681994 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:30:30,091 : INFO : PROGRESS: at 87.59% examples, 681010 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:31,038 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:30:31,040 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:30:31,051 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:30:31,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:30:31,052 : INFO : training on 5684315 raw words (5494277 effective words) took 8.0s, 684460 effective words/s\n",
      "2017-05-16 15:30:31,053 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:30:32,079 : INFO : PROGRESS: at 12.14% examples, 660046 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:33,097 : INFO : PROGRESS: at 24.45% examples, 662302 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:34,103 : INFO : PROGRESS: at 36.57% examples, 662480 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:35,128 : INFO : PROGRESS: at 49.42% examples, 668965 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 15:30:36,132 : INFO : PROGRESS: at 62.60% examples, 679457 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:37,140 : INFO : PROGRESS: at 75.28% examples, 681246 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:38,145 : INFO : PROGRESS: at 87.77% examples, 681403 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:39,057 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:30:39,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:30:39,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:30:39,071 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:30:39,072 : INFO : training on 5684315 raw words (5494414 effective words) took 8.0s, 686573 effective words/s\n",
      "2017-05-16 15:30:39,072 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:30:40,111 : INFO : PROGRESS: at 12.32% examples, 660658 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:41,137 : INFO : PROGRESS: at 25.50% examples, 683610 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:42,168 : INFO : PROGRESS: at 38.16% examples, 680680 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:43,168 : INFO : PROGRESS: at 50.64% examples, 682006 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 15:30:44,183 : INFO : PROGRESS: at 62.96% examples, 678908 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:45,193 : INFO : PROGRESS: at 76.32% examples, 686922 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:46,213 : INFO : PROGRESS: at 88.99% examples, 686226 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:47,035 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:30:47,037 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:30:47,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:30:47,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:30:47,052 : INFO : training on 5684315 raw words (5494484 effective words) took 8.0s, 689926 effective words/s\n",
      "2017-05-16 15:30:47,052 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:30:48,079 : INFO : PROGRESS: at 11.96% examples, 653697 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:49,107 : INFO : PROGRESS: at 24.97% examples, 674820 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:50,107 : INFO : PROGRESS: at 37.45% examples, 678529 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:51,108 : INFO : PROGRESS: at 49.59% examples, 675381 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:30:52,124 : INFO : PROGRESS: at 62.44% examples, 679188 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:30:53,148 : INFO : PROGRESS: at 75.62% examples, 683997 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:54,153 : INFO : PROGRESS: at 87.58% examples, 679680 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:55,081 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:30:55,089 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:30:55,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:30:55,103 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:30:55,104 : INFO : training on 5684315 raw words (5493469 effective words) took 8.0s, 684120 effective words/s\n",
      "2017-05-16 15:30:55,105 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:30:56,125 : INFO : PROGRESS: at 12.32% examples, 672775 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:30:57,145 : INFO : PROGRESS: at 25.32% examples, 686832 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:30:58,175 : INFO : PROGRESS: at 38.51% examples, 692571 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:30:59,178 : INFO : PROGRESS: at 51.71% examples, 699909 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:00,190 : INFO : PROGRESS: at 64.37% examples, 697370 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:01,214 : INFO : PROGRESS: at 77.55% examples, 699119 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:02,227 : INFO : PROGRESS: at 89.87% examples, 694610 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:02,982 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:02,989 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:02,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:03,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:03,006 : INFO : training on 5684315 raw words (5493423 effective words) took 7.9s, 696609 effective words/s\n",
      "2017-05-16 15:31:03,006 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:31:04,030 : INFO : PROGRESS: at 11.43% examples, 623075 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:05,038 : INFO : PROGRESS: at 24.27% examples, 661394 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:06,047 : INFO : PROGRESS: at 37.28% examples, 677184 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:07,056 : INFO : PROGRESS: at 50.12% examples, 682545 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:08,067 : INFO : PROGRESS: at 62.79% examples, 683748 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:09,070 : INFO : PROGRESS: at 75.45% examples, 685371 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:10,092 : INFO : PROGRESS: at 88.11% examples, 684730 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:11,001 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:11,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:11,014 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:11,014 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:11,015 : INFO : training on 5684315 raw words (5494379 effective words) took 8.0s, 687373 effective words/s\n",
      "2017-05-16 15:31:11,016 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:31:12,047 : INFO : PROGRESS: at 12.14% examples, 656400 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:13,064 : INFO : PROGRESS: at 24.62% examples, 665291 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:14,073 : INFO : PROGRESS: at 37.28% examples, 673402 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:15,079 : INFO : PROGRESS: at 49.42% examples, 670776 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:16,102 : INFO : PROGRESS: at 62.60% examples, 678384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:17,105 : INFO : PROGRESS: at 75.27% examples, 680889 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:31:18,119 : INFO : PROGRESS: at 88.29% examples, 684317 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:18,981 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:18,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:18,998 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:19,006 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:19,007 : INFO : training on 5684315 raw words (5494044 effective words) took 8.0s, 688859 effective words/s\n",
      "2017-05-16 15:31:19,007 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:31:20,036 : INFO : PROGRESS: at 12.15% examples, 658081 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:21,041 : INFO : PROGRESS: at 24.97% examples, 679669 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:22,045 : INFO : PROGRESS: at 37.10% examples, 674635 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:23,051 : INFO : PROGRESS: at 49.95% examples, 681356 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:24,077 : INFO : PROGRESS: at 61.90% examples, 672955 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:31:25,082 : INFO : PROGRESS: at 74.57% examples, 676147 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:26,093 : INFO : PROGRESS: at 87.59% examples, 680542 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:26,996 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:27,015 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:27,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:27,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:27,020 : INFO : training on 5684315 raw words (5494247 effective words) took 8.0s, 686999 effective words/s\n",
      "2017-05-16 15:31:27,021 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:31:28,044 : INFO : PROGRESS: at 11.96% examples, 656531 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:29,063 : INFO : PROGRESS: at 24.62% examples, 669743 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:30,086 : INFO : PROGRESS: at 37.28% examples, 673163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:31,094 : INFO : PROGRESS: at 47.49% examples, 644057 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:32,109 : INFO : PROGRESS: at 57.32% examples, 621896 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:33,133 : INFO : PROGRESS: at 70.16% examples, 633138 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:34,136 : INFO : PROGRESS: at 82.13% examples, 636179 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:35,160 : INFO : PROGRESS: at 94.97% examples, 642831 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:35,509 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:35,517 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:35,523 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:35,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:35,535 : INFO : training on 5684315 raw words (5494397 effective words) took 8.5s, 647021 effective words/s\n",
      "2017-05-16 15:31:35,535 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:31:36,575 : INFO : PROGRESS: at 12.50% examples, 669471 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:37,592 : INFO : PROGRESS: at 25.32% examples, 681420 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:38,608 : INFO : PROGRESS: at 38.33% examples, 688978 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:39,610 : INFO : PROGRESS: at 51.35% examples, 695083 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:40,628 : INFO : PROGRESS: at 64.19% examples, 694518 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:41,634 : INFO : PROGRESS: at 77.02% examples, 695617 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:42,638 : INFO : PROGRESS: at 89.34% examples, 692511 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:43,435 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:43,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:43,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:43,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:43,459 : INFO : training on 5684315 raw words (5493928 effective words) took 7.9s, 694651 effective words/s\n",
      "2017-05-16 15:31:43,460 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:31:44,491 : INFO : PROGRESS: at 11.79% examples, 637186 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:45,503 : INFO : PROGRESS: at 24.80% examples, 671851 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:46,510 : INFO : PROGRESS: at 37.28% examples, 675028 words/s, in_qsize 7, out_qsize 2\n",
      "2017-05-16 15:31:47,516 : INFO : PROGRESS: at 50.12% examples, 681413 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:48,529 : INFO : PROGRESS: at 62.43% examples, 678744 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:49,542 : INFO : PROGRESS: at 75.28% examples, 681704 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:50,543 : INFO : PROGRESS: at 87.41% examples, 679453 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:51,461 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:51,462 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:51,473 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:51,475 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:51,476 : INFO : training on 5684315 raw words (5493984 effective words) took 8.0s, 686715 effective words/s\n",
      "2017-05-16 15:31:51,477 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:31:52,505 : INFO : PROGRESS: at 12.50% examples, 680660 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:53,523 : INFO : PROGRESS: at 25.32% examples, 686918 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:31:54,541 : INFO : PROGRESS: at 36.92% examples, 666770 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:55,573 : INFO : PROGRESS: at 49.95% examples, 673382 words/s, in_qsize 7, out_qsize 2\n",
      "2017-05-16 15:31:56,584 : INFO : PROGRESS: at 63.31% examples, 683916 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:31:57,589 : INFO : PROGRESS: at 75.62% examples, 682153 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:31:58,606 : INFO : PROGRESS: at 88.65% examples, 685078 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:31:59,435 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:31:59,438 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:31:59,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:31:59,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:31:59,462 : INFO : training on 5684315 raw words (5494088 effective words) took 8.0s, 689824 effective words/s\n",
      "2017-05-16 15:31:59,463 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:00,494 : INFO : PROGRESS: at 12.32% examples, 666683 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:01,512 : INFO : PROGRESS: at 25.15% examples, 679803 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:32:02,525 : INFO : PROGRESS: at 37.81% examples, 682017 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:03,532 : INFO : PROGRESS: at 50.64% examples, 686651 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:32:04,546 : INFO : PROGRESS: at 62.43% examples, 677062 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:05,556 : INFO : PROGRESS: at 75.28% examples, 680630 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:06,575 : INFO : PROGRESS: at 88.12% examples, 682289 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:07,446 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:32:07,451 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:32:07,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:32:07,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:32:07,461 : INFO : training on 5684315 raw words (5494223 effective words) took 8.0s, 688356 effective words/s\n",
      "2017-05-16 15:32:07,462 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:08,483 : INFO : PROGRESS: at 11.79% examples, 644058 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:09,486 : INFO : PROGRESS: at 24.62% examples, 673380 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:10,504 : INFO : PROGRESS: at 37.28% examples, 676867 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:11,522 : INFO : PROGRESS: at 50.30% examples, 683209 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:12,549 : INFO : PROGRESS: at 62.77% examples, 680099 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:13,550 : INFO : PROGRESS: at 75.45% examples, 682567 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:14,551 : INFO : PROGRESS: at 87.06% examples, 676204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:15,499 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:32:15,508 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:32:15,517 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:32:15,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:32:15,524 : INFO : training on 5684315 raw words (5493877 effective words) took 8.0s, 682764 effective words/s\n",
      "2017-05-16 15:32:15,525 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:16,562 : INFO : PROGRESS: at 12.50% examples, 672537 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:32:17,583 : INFO : PROGRESS: at 25.50% examples, 686175 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:18,600 : INFO : PROGRESS: at 38.33% examples, 688829 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:19,621 : INFO : PROGRESS: at 51.53% examples, 694081 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:20,646 : INFO : PROGRESS: at 64.54% examples, 694783 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:21,665 : INFO : PROGRESS: at 77.72% examples, 697502 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:22,690 : INFO : PROGRESS: at 90.40% examples, 694824 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:23,358 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:32:23,379 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:32:23,386 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:32:23,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:32:23,392 : INFO : training on 5684315 raw words (5494355 effective words) took 7.9s, 699871 effective words/s\n",
      "2017-05-16 15:32:23,393 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:24,447 : INFO : PROGRESS: at 11.61% examples, 617524 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:25,447 : INFO : PROGRESS: at 24.80% examples, 670039 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:26,459 : INFO : PROGRESS: at 37.63% examples, 679113 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:32:27,459 : INFO : PROGRESS: at 50.12% examples, 680706 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:32:28,461 : INFO : PROGRESS: at 62.61% examples, 681568 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:29,467 : INFO : PROGRESS: at 75.45% examples, 684875 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:30,488 : INFO : PROGRESS: at 87.76% examples, 681595 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:31,388 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:32:31,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:32:31,404 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:32:31,406 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:32:31,407 : INFO : training on 5684315 raw words (5494290 effective words) took 8.0s, 687396 effective words/s\n",
      "2017-05-16 15:32:31,408 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:32,454 : INFO : PROGRESS: at 12.14% examples, 649004 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 15:32:33,458 : INFO : PROGRESS: at 25.14% examples, 680281 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:34,459 : INFO : PROGRESS: at 37.10% examples, 672590 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:35,459 : INFO : PROGRESS: at 49.78% examples, 678233 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:32:36,460 : INFO : PROGRESS: at 62.60% examples, 683437 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:37,469 : INFO : PROGRESS: at 75.27% examples, 684495 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:38,499 : INFO : PROGRESS: at 88.47% examples, 687216 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:39,292 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:32:39,314 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:32:39,314 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:32:39,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:32:39,316 : INFO : training on 5684315 raw words (5494106 effective words) took 7.9s, 696452 effective words/s\n",
      "2017-05-16 15:32:39,317 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:40,343 : INFO : PROGRESS: at 12.14% examples, 663685 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:41,346 : INFO : PROGRESS: at 24.97% examples, 683514 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:42,350 : INFO : PROGRESS: at 37.45% examples, 683452 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:43,351 : INFO : PROGRESS: at 49.95% examples, 683877 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:44,358 : INFO : PROGRESS: at 61.73% examples, 675696 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:45,375 : INFO : PROGRESS: at 74.74% examples, 680284 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:46,386 : INFO : PROGRESS: at 87.59% examples, 682737 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:47,285 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:32:47,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:32:47,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:32:47,306 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:32:47,307 : INFO : training on 5684315 raw words (5493781 effective words) took 8.0s, 689483 effective words/s\n",
      "2017-05-16 15:32:47,307 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:48,334 : INFO : PROGRESS: at 12.50% examples, 683124 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:49,349 : INFO : PROGRESS: at 24.97% examples, 679317 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:32:50,370 : INFO : PROGRESS: at 37.80% examples, 683289 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:51,401 : INFO : PROGRESS: at 50.47% examples, 681036 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:52,402 : INFO : PROGRESS: at 63.48% examples, 687609 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:53,415 : INFO : PROGRESS: at 75.96% examples, 685885 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:54,436 : INFO : PROGRESS: at 87.94% examples, 679832 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:55,325 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:32:55,327 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:32:55,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:32:55,341 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:32:55,342 : INFO : training on 5684315 raw words (5494222 effective words) took 8.0s, 685692 effective words/s\n",
      "2017-05-16 15:32:55,343 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:32:56,383 : INFO : PROGRESS: at 12.14% examples, 651988 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:32:57,388 : INFO : PROGRESS: at 24.62% examples, 666991 words/s, in_qsize 7, out_qsize 2\n",
      "2017-05-16 15:32:58,396 : INFO : PROGRESS: at 38.16% examples, 690605 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:32:59,398 : INFO : PROGRESS: at 50.64% examples, 689152 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:33:00,407 : INFO : PROGRESS: at 63.66% examples, 692931 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:01,409 : INFO : PROGRESS: at 76.50% examples, 694793 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:02,428 : INFO : PROGRESS: at 88.81% examples, 690368 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:03,238 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:03,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:03,251 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:03,252 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:03,253 : INFO : training on 5684315 raw words (5494046 effective words) took 7.9s, 696085 effective words/s\n",
      "2017-05-16 15:33:03,254 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:33:04,295 : INFO : PROGRESS: at 11.44% examples, 612224 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:05,300 : INFO : PROGRESS: at 24.80% examples, 670722 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:06,307 : INFO : PROGRESS: at 36.93% examples, 668035 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 15:33:07,314 : INFO : PROGRESS: at 50.12% examples, 680853 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:08,317 : INFO : PROGRESS: at 62.60% examples, 681403 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:33:09,335 : INFO : PROGRESS: at 75.45% examples, 683393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:10,337 : INFO : PROGRESS: at 88.11% examples, 684880 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:33:11,238 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:11,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:11,246 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:11,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:11,254 : INFO : training on 5684315 raw words (5494489 effective words) took 8.0s, 688113 effective words/s\n",
      "2017-05-16 15:33:11,255 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:33:12,281 : INFO : PROGRESS: at 12.32% examples, 673223 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:13,299 : INFO : PROGRESS: at 25.14% examples, 683040 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:14,323 : INFO : PROGRESS: at 36.93% examples, 665911 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:33:15,342 : INFO : PROGRESS: at 49.95% examples, 674852 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:16,364 : INFO : PROGRESS: at 62.96% examples, 679855 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:17,369 : INFO : PROGRESS: at 75.62% examples, 681862 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:18,376 : INFO : PROGRESS: at 88.47% examples, 684548 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:19,225 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:19,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:19,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:19,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:19,249 : INFO : training on 5684315 raw words (5493546 effective words) took 8.0s, 689108 effective words/s\n",
      "2017-05-16 15:33:19,249 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:33:20,290 : INFO : PROGRESS: at 12.50% examples, 672115 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:21,299 : INFO : PROGRESS: at 25.50% examples, 690023 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:22,301 : INFO : PROGRESS: at 37.63% examples, 682101 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:23,312 : INFO : PROGRESS: at 50.30% examples, 683560 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:24,318 : INFO : PROGRESS: at 62.60% examples, 681315 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:25,321 : INFO : PROGRESS: at 75.62% examples, 686527 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:26,335 : INFO : PROGRESS: at 88.29% examples, 686449 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:27,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:27,198 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:27,199 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:27,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:27,201 : INFO : training on 5684315 raw words (5494209 effective words) took 7.9s, 692726 effective words/s\n",
      "2017-05-16 15:33:27,202 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:33:28,222 : INFO : PROGRESS: at 11.96% examples, 654300 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:29,225 : INFO : PROGRESS: at 24.62% examples, 673775 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:30,249 : INFO : PROGRESS: at 37.28% examples, 675993 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:31,267 : INFO : PROGRESS: at 49.78% examples, 675454 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:32,279 : INFO : PROGRESS: at 62.43% examples, 677705 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:33:33,301 : INFO : PROGRESS: at 75.27% examples, 679816 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:34,308 : INFO : PROGRESS: at 87.76% examples, 680034 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:35,218 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:35,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:35,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:35,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:35,242 : INFO : training on 5684315 raw words (5494352 effective words) took 8.0s, 684776 effective words/s\n",
      "2017-05-16 15:33:35,242 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:33:36,262 : INFO : PROGRESS: at 12.32% examples, 673033 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:37,265 : INFO : PROGRESS: at 25.32% examples, 692846 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:38,272 : INFO : PROGRESS: at 37.98% examples, 692153 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:33:39,278 : INFO : PROGRESS: at 51.00% examples, 696977 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:33:40,287 : INFO : PROGRESS: at 63.48% examples, 693558 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:41,292 : INFO : PROGRESS: at 76.32% examples, 694935 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:42,299 : INFO : PROGRESS: at 88.99% examples, 694378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:43,131 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:43,142 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:43,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:43,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:43,157 : INFO : training on 5684315 raw words (5494625 effective words) took 7.9s, 695517 effective words/s\n",
      "2017-05-16 15:33:43,158 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:33:44,208 : INFO : PROGRESS: at 11.61% examples, 616932 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:45,219 : INFO : PROGRESS: at 24.79% examples, 666197 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:46,220 : INFO : PROGRESS: at 37.80% examples, 682085 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:47,228 : INFO : PROGRESS: at 50.46% examples, 683995 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:48,254 : INFO : PROGRESS: at 63.49% examples, 686535 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:49,255 : INFO : PROGRESS: at 76.14% examples, 687956 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:33:50,256 : INFO : PROGRESS: at 88.65% examples, 687599 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:33:51,103 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:51,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:51,120 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:51,125 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:51,126 : INFO : training on 5684315 raw words (5493558 effective words) took 8.0s, 690861 effective words/s\n",
      "2017-05-16 15:33:51,127 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:33:52,153 : INFO : PROGRESS: at 12.14% examples, 659619 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:53,157 : INFO : PROGRESS: at 24.79% examples, 676323 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:54,165 : INFO : PROGRESS: at 36.75% examples, 668157 words/s, in_qsize 7, out_qsize 2\n",
      "2017-05-16 15:33:55,179 : INFO : PROGRESS: at 49.78% examples, 677412 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:56,192 : INFO : PROGRESS: at 62.79% examples, 683112 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:33:57,199 : INFO : PROGRESS: at 75.96% examples, 689197 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:58,202 : INFO : PROGRESS: at 88.99% examples, 692524 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:33:59,022 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:33:59,022 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:33:59,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:33:59,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:33:59,035 : INFO : training on 5684315 raw words (5494117 effective words) took 7.9s, 696075 effective words/s\n",
      "2017-05-16 15:33:59,036 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:00,058 : INFO : PROGRESS: at 12.32% examples, 671813 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:01,078 : INFO : PROGRESS: at 25.32% examples, 686444 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:34:02,083 : INFO : PROGRESS: at 37.63% examples, 681970 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:03,099 : INFO : PROGRESS: at 50.47% examples, 685032 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:04,110 : INFO : PROGRESS: at 62.43% examples, 678005 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:05,131 : INFO : PROGRESS: at 75.45% examples, 681738 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 15:34:06,147 : INFO : PROGRESS: at 88.64% examples, 686312 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:06,976 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:34:06,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:34:06,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:34:06,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:34:06,996 : INFO : training on 5684315 raw words (5494418 effective words) took 7.9s, 691536 effective words/s\n",
      "2017-05-16 15:34:06,997 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:08,015 : INFO : PROGRESS: at 11.96% examples, 654910 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:09,015 : INFO : PROGRESS: at 24.80% examples, 679965 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:10,024 : INFO : PROGRESS: at 37.63% examples, 686275 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:11,043 : INFO : PROGRESS: at 50.46% examples, 687933 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:12,056 : INFO : PROGRESS: at 63.31% examples, 689556 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:13,077 : INFO : PROGRESS: at 75.96% examples, 688144 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:14,090 : INFO : PROGRESS: at 88.29% examples, 685291 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:14,945 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:34:14,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:34:14,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:34:14,968 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:34:14,969 : INFO : training on 5684315 raw words (5493943 effective words) took 8.0s, 690483 effective words/s\n",
      "2017-05-16 15:34:14,969 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:15,993 : INFO : PROGRESS: at 12.32% examples, 675618 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:17,007 : INFO : PROGRESS: at 25.32% examples, 690499 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:18,008 : INFO : PROGRESS: at 38.51% examples, 701406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:19,025 : INFO : PROGRESS: at 51.18% examples, 697098 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:20,053 : INFO : PROGRESS: at 64.01% examples, 694831 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:21,056 : INFO : PROGRESS: at 76.85% examples, 696223 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:22,062 : INFO : PROGRESS: at 89.35% examples, 694209 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:34:22,843 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:34:22,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:34:22,858 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:34:22,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:34:22,860 : INFO : training on 5684315 raw words (5493738 effective words) took 7.9s, 698241 effective words/s\n",
      "2017-05-16 15:34:22,861 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:23,896 : INFO : PROGRESS: at 11.79% examples, 635070 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:24,897 : INFO : PROGRESS: at 24.62% examples, 669516 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:25,902 : INFO : PROGRESS: at 37.45% examples, 680381 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:26,906 : INFO : PROGRESS: at 50.64% examples, 690661 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:27,907 : INFO : PROGRESS: at 63.31% examples, 691506 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:28,910 : INFO : PROGRESS: at 75.96% examples, 691818 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:29,926 : INFO : PROGRESS: at 88.64% examples, 690794 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:30,774 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:34:30,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:34:30,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:34:30,799 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:34:30,800 : INFO : training on 5684315 raw words (5494124 effective words) took 7.9s, 693404 effective words/s\n",
      "2017-05-16 15:34:30,801 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:31,823 : INFO : PROGRESS: at 12.50% examples, 681932 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:32,831 : INFO : PROGRESS: at 25.15% examples, 685854 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:33,835 : INFO : PROGRESS: at 37.28% examples, 678677 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:34,837 : INFO : PROGRESS: at 49.59% examples, 677728 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:35,870 : INFO : PROGRESS: at 62.78% examples, 682521 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:34:36,888 : INFO : PROGRESS: at 75.97% examples, 687525 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:37,891 : INFO : PROGRESS: at 89.17% examples, 692397 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:38,683 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:34:38,684 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:34:38,694 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:34:38,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:34:38,696 : INFO : training on 5684315 raw words (5493741 effective words) took 7.9s, 697204 effective words/s\n",
      "2017-05-16 15:34:38,697 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:39,746 : INFO : PROGRESS: at 12.67% examples, 678399 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:40,763 : INFO : PROGRESS: at 25.67% examples, 690555 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:41,780 : INFO : PROGRESS: at 38.85% examples, 697798 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:42,785 : INFO : PROGRESS: at 51.18% examples, 691615 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:43,813 : INFO : PROGRESS: at 63.31% examples, 682972 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:44,815 : INFO : PROGRESS: at 75.96% examples, 684847 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:45,841 : INFO : PROGRESS: at 89.17% examples, 687903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:46,629 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:34:46,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:34:46,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:34:46,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:34:46,653 : INFO : training on 5684315 raw words (5494074 effective words) took 7.9s, 692565 effective words/s\n",
      "2017-05-16 15:34:46,653 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:47,677 : INFO : PROGRESS: at 12.50% examples, 680273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:48,710 : INFO : PROGRESS: at 24.97% examples, 671905 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:49,728 : INFO : PROGRESS: at 38.33% examples, 688547 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:50,738 : INFO : PROGRESS: at 50.64% examples, 683883 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:51,751 : INFO : PROGRESS: at 63.66% examples, 688258 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 15:34:52,758 : INFO : PROGRESS: at 76.50% examples, 690200 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:53,777 : INFO : PROGRESS: at 89.17% examples, 689089 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:54,582 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:34:54,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:34:54,604 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:34:54,605 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:34:54,606 : INFO : training on 5684315 raw words (5493950 effective words) took 7.9s, 692167 effective words/s\n",
      "2017-05-16 15:34:54,607 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:34:55,634 : INFO : PROGRESS: at 12.32% examples, 669469 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:34:56,642 : INFO : PROGRESS: at 25.14% examples, 684503 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:57,650 : INFO : PROGRESS: at 38.16% examples, 692784 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:58,680 : INFO : PROGRESS: at 51.35% examples, 695530 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:34:59,693 : INFO : PROGRESS: at 64.02% examples, 693705 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:35:00,707 : INFO : PROGRESS: at 76.67% examples, 692464 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:35:01,720 : INFO : PROGRESS: at 89.86% examples, 695772 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:35:02,465 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:35:02,466 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:35:02,466 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:35:02,483 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:35:02,483 : INFO : training on 5684315 raw words (5494004 effective words) took 7.9s, 699016 effective words/s\n",
      "2017-05-16 15:35:02,484 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:35:03,527 : INFO : PROGRESS: at 12.50% examples, 667293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:35:04,539 : INFO : PROGRESS: at 24.80% examples, 667841 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:35:05,549 : INFO : PROGRESS: at 37.98% examples, 684140 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:35:06,573 : INFO : PROGRESS: at 51.36% examples, 692456 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:35:07,593 : INFO : PROGRESS: at 64.54% examples, 696071 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:35:08,607 : INFO : PROGRESS: at 77.37% examples, 695991 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:35:09,621 : INFO : PROGRESS: at 88.99% examples, 686448 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:35:10,491 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:35:10,493 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:35:10,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:35:10,496 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:35:10,496 : INFO : training on 5684315 raw words (5494104 effective words) took 8.0s, 686963 effective words/s\n",
      "2017-05-16 15:35:10,497 : INFO : training model with 4 workers on 82063 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 15:35:11,522 : INFO : PROGRESS: at 11.96% examples, 651875 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:35:12,526 : INFO : PROGRESS: at 24.09% examples, 657773 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 15:35:13,553 : INFO : PROGRESS: at 36.58% examples, 661353 words/s, in_qsize 7, out_qsize 3\n",
      "2017-05-16 15:35:14,557 : INFO : PROGRESS: at 49.25% examples, 669166 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:35:15,563 : INFO : PROGRESS: at 61.73% examples, 671703 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 15:35:16,574 : INFO : PROGRESS: at 74.21% examples, 672835 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 15:35:17,579 : INFO : PROGRESS: at 87.41% examples, 679656 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 15:35:18,487 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 15:35:18,493 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 15:35:18,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 15:35:18,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 15:35:18,509 : INFO : training on 5684315 raw words (5493831 effective words) took 8.0s, 687168 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1,size=50,workers=4)\n",
    "for i in range(50):\n",
    "    print('epcoch',i)\n",
    "    model.train(sentences,total_examples=model.corpus_count,epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2vec_features(model,sentences):\n",
    "    'sentences :  [[token1,...,tokend],..,[token1,...,tokend]]'\n",
    "    'model : Trained Word 2 Vec model'\n",
    "    \n",
    "    max_distance_tokens = []\n",
    "    min_distance_tokens_duplic_removed = []\n",
    "    min_distances = []\n",
    "    centroid_distances = []\n",
    "\n",
    "\n",
    "    for i in range(int(len(sentences)/2)):\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        d_min_classic = 1\n",
    "        d_min = 1\n",
    "        d_max = -1\n",
    "        distance_centroid = -1\n",
    "        \n",
    "        set1 = set(sentences[2*i])\n",
    "        set2 = set(sentences[2*i+1])\n",
    "        \n",
    "        sym_dif = set1.symmetric_difference(set2)\n",
    "    \n",
    "        #in the else condition we assume set1 and set2 non empty\n",
    "        if len(set1)>0 and len(set2) >0:\n",
    "            for token1 in set1:\n",
    "                for token2 in set2:\n",
    "                    distance_ = cosine_similarity(model[token1].reshape(1,-1),model[token2].reshape(1,-1))[0,0]\n",
    "                    if distance_ <= d_min_classic:\n",
    "                        d_min_classic = distance_\n",
    "                    \n",
    "        #following condition is same with adding len(set2)>0\n",
    "            #means set1==set2 and non empty sets\n",
    "            if len(sym_dif) == 0:\n",
    "                d_min = 1\n",
    "                d_max = 1\n",
    "            else:\n",
    "                if set1.issubset(set2) or set2.issubset(set1):\n",
    "                    d_min = -1\n",
    "                    d_max = -1\n",
    "                else:\n",
    "                    for token1 in set1&sym_dif:\n",
    "                        for token2 in set2&sym_dif:\n",
    "                            distance_tokens =  cosine_similarity(model[token1].reshape(1,-1),model[token2].reshape(1,-1))[0,0]\n",
    "                        if distance_tokens <= d_min:\n",
    "                            d_min = distance_tokens\n",
    "                        if distance_tokens >= d_max:\n",
    "                            d_max =distance_tokens\n",
    "        \n",
    "        if min(len(set1),len(set2))>0:\n",
    "            centroid1 = np.sum([model[token1] for token1 in set1],axis=0)/len(set1)\n",
    "            centroid2 = np.sum([model[token2] for token2 in set2],axis=0)/len(set2)\n",
    "            distance_centroid = cosine_similarity(centroid1.reshape(1,-1),centroid2.reshape(1,-1))[0,0]\n",
    "            \n",
    "        max_distance_tokens.append(d_max)\n",
    "        min_distances.append(d_min_classic)\n",
    "        min_distance_tokens_duplic_removed.append(d_min)\n",
    "        centroid_distances.append(distance_centroid)\n",
    "    \n",
    "    word2vec_features = np.array([centroid_distances,min_distances,min_distance_tokens_duplic_removed,max_distance_tokens]).T\n",
    "                                            \n",
    "    return word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n"
     ]
    }
   ],
   "source": [
    "w2vec_feat = word2vec_features(model,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n",
      "0.715086\n",
      "0.472071\n"
     ]
    }
   ],
   "source": [
    "print((model['bad'].reshape(1,-1)).shape)\n",
    "print(cosine_similarity(model['great'].reshape(1,-1),model['good'].reshape(1,-1))[0,0])\n",
    "print(cosine_similarity(model['bad'].reshape(1,-1),model['good'].reshape(1,-1))[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [ 0.94257027 -0.12202989  0.52574468  0.57970071] 0 \n",
      " ['step', 'step', 'guid', 'invest', 'share', 'market', 'india?'] \n",
      " ['step', 'step', 'guid', 'invest', 'share', 'market?']\n",
      "\n",
      " [ 0.66128707 -0.2324957  -0.03917172  0.03197373] 0 \n",
      " ['stori', 'kohinoor', '(koh-i-noor)', 'diamond?'] \n",
      " ['would', 'happen', 'indian', 'govern', 'stole', 'kohinoor', '(koh-i-noor)', 'diamond', 'back?']\n",
      "\n",
      " [ 0.8215493  -0.2996707   0.02198204  0.29375705] 0 \n",
      " ['increas', 'speed', 'internet', 'connect', 'use', 'vpn?'] \n",
      " ['internet', 'speed', 'increas', 'hack', 'dns?']\n",
      "\n",
      " [ 0.05164458 -0.15548569 -0.15548569  0.33869255] 0 \n",
      " ['mental', 'lonely?', 'solv', 'it?'] \n",
      " ['find', 'remaind', '[math]23^{24}[/math]', 'divid', '24,23?']\n",
      "\n",
      " [ 0.55903596 -0.24283861 -0.18909988  0.46265078] 0 \n",
      " ['one', 'dissolv', 'water', 'quikli', 'sugar,', 'salt,', 'methan', 'carbon', 'di', 'oxide?'] \n",
      " ['fish', 'would', 'surviv', 'salt', 'water?']\n",
      "\n",
      " [ 0.80918324 -0.27047479  0.20256272  0.76206708] 1 \n",
      " ['astrology:', 'capricorn', 'sun', 'cap', 'moon', 'cap', 'rising...what', 'say', 'me?'] \n",
      " [\"i'm\", 'tripl', 'capricorn', '(sun,', 'moon', 'ascend', 'capricorn)', 'say', 'me?']\n",
      "\n",
      " [ 0.0948433  -0.28726476  0.1953899   0.21138273] 0 \n",
      " ['buy', 'tiago?'] \n",
      " ['keep', 'childern', 'activ', 'far', 'phone', 'video', 'games?']\n",
      "\n",
      " [ 0.72954744 -0.21212474  0.71508622  0.71508622] 1 \n",
      " ['good', 'geologist?'] \n",
      " ['great', 'geologist?']\n",
      "\n",
      " [ 0.93068552 -0.31094882  0.83296758  0.83740425] 0 \n",
      " ['use', 'シ', 'instead', 'し?'] \n",
      " ['use', '\"&\"', 'instead', '\"and\"?']\n",
      "\n",
      " [ 0.57580447 -0.08055607 -0.08055606  0.00188224] 0 \n",
      " ['motorola', '(company):', 'hack', 'charter', 'motorolla', 'dcx3400?'] \n",
      " ['hack', 'motorola', 'dcx3400', 'free', 'internet?']\n",
      "\n",
      " [-0.01452423 -0.42709175 -0.17420498  0.22249101] 0 \n",
      " ['method', 'find', 'separ', 'slit', 'use', 'fresnel', 'biprism?'] \n",
      " ['thing', 'technician', 'tell', 'durabl', 'reliabl', 'laptop', 'components?']\n",
      "\n",
      " [ 0.88812315 -0.17525938  0.41561538  0.58297092] 1 \n",
      " ['read', 'find', 'youtub', 'comments?'] \n",
      " ['see', 'youtub', 'comments?']\n",
      "\n",
      " [ 0.99999994 -0.15540746  1.          1.        ] 1 \n",
      " ['make', 'physic', 'easi', 'learn?'] \n",
      " ['make', 'physic', 'easi', 'learn?']\n",
      "\n",
      " [ 0.82950842 -0.26679119  0.30964628  0.75522178] 1 \n",
      " ['first', 'sexual', 'experi', 'like?'] \n",
      " ['first', 'sexual', 'experience?']\n",
      "\n",
      " [ 0.98342675 -0.164938    0.48562014  0.73860073] 0 \n",
      " ['law', 'chang', 'statu', 'student', 'visa', 'green', 'card', 'us,', 'compar', 'immigr', 'law', 'canada?'] \n",
      " ['law', 'chang', 'statu', 'student', 'visa', 'green', 'card', 'us?', 'compar', 'immigr', 'law', 'japan?']\n",
      "\n",
      " [ 0.88415831 -0.26208985 -0.2314921   0.43743515] 1 \n",
      " ['would', 'trump', 'presid', 'mean', 'current', 'intern', 'master’', 'student', 'f1', 'visa?'] \n",
      " ['trump', 'presid', 'affect', 'student', 'present', 'us', 'plan', 'studi', 'us?']\n",
      "\n",
      " [ 0.68337423 -0.11430845  0.45236099  0.45236099] 1 \n",
      " ['manipul', 'mean?'] \n",
      " ['manipul', 'means?']\n",
      "\n",
      " [ 0.85372078 -0.00963618  0.00653565  0.38541746] 0 \n",
      " ['girl', 'want', 'friend', 'guy', 'reject?'] \n",
      " ['guy', 'feel', 'reject', 'girl?']\n",
      "\n",
      " [ 0.8705467  -0.18272853 -0.04956734  0.30194837] 1 \n",
      " ['mani', 'quora', 'user', 'post', 'question', 'readili', 'answer', 'google?'] \n",
      " ['peopl', 'ask', 'quora', 'question', 'answer', 'easili', 'google?']\n",
      "\n",
      " [ 0.95278716  0.0725083   0.52242041  0.52242041] 0 \n",
      " ['best', 'digit', 'market', 'institut', 'banglore?'] \n",
      " ['best', 'digit', 'market', 'institut', 'pune?']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('\\n',w2vec_feat[i,:],labels[i],'\\n',sentences[2*i],'\\n',sentences[2*i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def doc2vecs_features(sentences_train,nb_epochs=100,alpha=0.025,min_alpha=0.025):\n",
    "#     sentences = []\n",
    "#     stemmer = PorterStemmer()\n",
    "#     for i in range(sentences_train.shape[0]): \n",
    "#         source_sentence = sentences_train[i,0].lower().split(\" \") \n",
    "#         source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "#         unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "#         sentences.append(unigrams_que1)\n",
    "\n",
    "#         target_sentence = sentences_train[i,1].lower().split(\" \") \n",
    "#         target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "#         unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "#         sentences.append(unigrams_que2)\n",
    "\n",
    "#     texts=sentences.copy()\n",
    "#     documents = []\n",
    "#     ct = 0\n",
    "#     for doc in texts:\n",
    "#         doc = gensim.models.doc2vec.LabeledSentence(words = doc, tags = ['SENT_'+str(ct)])\n",
    "#         ct+=1\n",
    "#         documents.append(doc)\n",
    "#     model = gensim.models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1,workers=4)\n",
    "#     model.build_vocab(documents)\n",
    "\n",
    "#     for epoch in range(100):\n",
    "#         model.train(documents,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        \n",
    "#     most_similar_is_duo_1_2 = []\n",
    "#     most_similar_is_duo_2_1 = []\n",
    "    \n",
    "#     most_similar_score_if_duo_1_2 = []\n",
    "#     most_similar_score_if_duo_2_1 = []\n",
    "    \n",
    "#     n_similarities = []\n",
    "    \n",
    "#     for i in range(sentences_train.shape[0]):\n",
    "        \n",
    "#         most_sim_1 = model.docvecs.most_similar([\"SENT_\"+str(2*i)])[0]\n",
    "#         most_sim_2 = model.docvecs.most_similar([\"SENT_\"+str(2*i+1)])[0]\n",
    "        \n",
    "#         most_similar_is_duo_1_2.append(int(most_sim_1[0] == \"SENT_\"+str(2*i+1)))\n",
    "#         most_similar_is_duo_2_1.append(int(most_sim_2[0] == \"SENT_\"+str(2*i)))\n",
    "        \n",
    "#         most_similar_score_if_duo_1_2.append(int(most_sim_1[0] == \"SENT_\"+str(2*i+1))*most_sim_1[1])\n",
    "#         most_similar_score_if_duo_2_1.append(int(most_sim_2[0] == \"SENT_\"+str(2*i))*most_sim_2[1])\n",
    "        \n",
    "#         n_similarities.append(model.n_similarity(sentences[2*i], sentences[2*i+1]))\n",
    "                                             \n",
    "#         doc_2_vec_features = np.array([n_similarities,\n",
    "#                                        most_similar_score_if_duo_1_2,\n",
    "#                                        most_similar_score_if_duo_2_1,\n",
    "#                                        most_similar_is_duo_1_2,\n",
    "#                                        most_similar_is_duo_2_1])\n",
    "                                            \n",
    "#     return doc_2_vec_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and word sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(size_train)\n",
    "df_train = Training_Set.iloc[:size_train]\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    return 0 if count < min_count else 1 / (count + eps)\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "stops = stpwds\n",
    "\n",
    "def word_shares(row):\n",
    "    q1 = set(str(row['question1']).lower().split())\n",
    "    q1words = q1.difference(stops)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0'\n",
    "\n",
    "    q2 = set(str(row['question2']).lower().split())\n",
    "    q2words = q2.difference(stops)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0'\n",
    "\n",
    "    q1stops = q1.intersection(stops)\n",
    "    q2stops = q2.intersection(stops)\n",
    "\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "    shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    if len(shared_words) > 0:\n",
    "        R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "    else:\n",
    "        R1 = 0\n",
    "    R2 = len(shared_words) / (len(q1words) + len(q2words)) #count share\n",
    "    R31 = len(q1stops) / len(q1words) #stops in q1\n",
    "    R32 = len(q2stops) / len(q2words) #stops in q2\n",
    "    return '{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascalsitbon/anaconda/envs/EnvDevMachineLearning/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df=df_train\n",
    "df['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n",
    "\n",
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
    "\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
    "\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
    "\n",
    "x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
    "\n",
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "x['duplicated'] = df.duplicated(['question1','question2']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and Hash Tag ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#frquency questions and hash.\n",
    "train_orig =  pd.read_csv('train.csv', header=0)\n",
    "test_orig =  pd.read_csv('test.csv', header=0)\n",
    "\n",
    "tic0=timeit.default_timer()\n",
    "df1 = train_orig[['question1']].copy()\n",
    "df2 = train_orig[['question2']].copy()\n",
    "df1_test = test_orig[['question1']].copy()\n",
    "df2_test = test_orig[['question2']].copy()\n",
    "\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions = train_questions.append(df1_test)\n",
    "train_questions = train_questions.append(df2_test)\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "train_cp = train_orig.copy()\n",
    "test_cp = test_orig.copy()\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "test_cp['is_duplicate'] = -1\n",
    "test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_features(data_set,nb_ex):\n",
    "    # Select basic features    \n",
    "    stemmer = PorterStemmer()\n",
    "    lens1 = []\n",
    "    lens2 = []\n",
    "    common_unigrams_lens = []\n",
    "    common_unigrams_ratios = []\n",
    "    common_bigrams_lens = []\n",
    "    common_bigrams_ratios = []\n",
    "    common_trigrams_lens = []\n",
    "    dif_len = []\n",
    "    common_trigrams_ratios = []\n",
    "    for i in range(nb_ex):\n",
    "        if i%10000== 0:\n",
    "            print(i)\n",
    "        source_sentence = data_set[i,0].lower().split(\" \") \n",
    "        source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "        unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "        \n",
    "        target_sentence = data_set[i,1].lower().split(\" \") \n",
    "        target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "        unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "\n",
    "        #get unigram features #\n",
    "        common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
    "        common_unigrams_lens.append(common_unigrams_len)\n",
    "        common_unigrams_ratios.append(float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1))\n",
    "        \n",
    "        # get bigram features #\n",
    "        bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n",
    "        bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n",
    "        common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n",
    "        common_bigrams_lens.append(common_bigrams_len)\n",
    "        common_bigrams_ratios.append(float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1))\n",
    "\n",
    "\n",
    "        # get trigram features #\n",
    "        trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n",
    "        trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n",
    "        common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n",
    "        common_trigrams_lens.append(common_trigrams_len)\n",
    "        common_trigrams_ratios.append(float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1))\n",
    "        \n",
    "        lens1.append(len(source_sentence))\n",
    "        lens2.append(len(target_sentence))\n",
    "        dif_len.append(abs(len(source_sentence)-len(source_sentence)))\n",
    "    \n",
    "    features = np.array([common_unigrams_lens,\n",
    "                         common_unigrams_ratios,\n",
    "                         common_bigrams_lens,\n",
    "                         common_bigrams_ratios,\n",
    "                         common_trigrams_lens,\n",
    "                         common_trigrams_ratios,\n",
    "                         lens1,\n",
    "                         lens2,\n",
    "                         dif_len]).T\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "n_grams_features,y= select_features(sentences_train,size_train),labels[:size_train].astype(float)#,lsi,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqhash_features = train_comb[['q1_hash','q2_hash','q1_freq','q2_freq']].iloc[:size_train].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_shares = x.values\n",
    "word_shares_names = x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(word_shares.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascalsitbon/anaconda/envs/EnvDevMachineLearning/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "X = np.column_stack([n_grams_features,freqhash_features,w2vec_feat,word_shares])\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.331654411544\n",
      "Testing Score: 0.347173140622\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,max_depth = 9,max_features=12)\n",
    "clf.fit(X_train,y_train)\n",
    "print('Training Score:',sklearn.metrics.log_loss(y_train,clf.predict_proba(X_train)[:,1]))\n",
    "print('Testing Score:',sklearn.metrics.log_loss(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=9, max_features=12, max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [3, 7], 'max_features': [4, 7, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='log_loss', verbose=0)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"max_depth\": [5, 7 ,10],\n",
    "              \"max_features\": [6,, 10]}\n",
    "\n",
    "# run grid search\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid,scoring='log_loss')\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.359160412172\n",
      "Testing Score: 0.362573138925\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "clf.fit(X_train,y_train)\n",
    "print('Training Score:',sklearn.metrics.log_loss(y_train,clf.predict_proba(X_train)[:,1]))\n",
    "print('Testing Score:',sklearn.metrics.log_loss(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "(1, 'tfidf_word_match', 0.26751357726060332)\n",
      "(2, 'word_match', 0.14024605785405214)\n",
      "(3, 'q1_freq', 0.1344186887150203)\n",
      "(4, 'common_unigrams_ratios', 0.11996747872219837)\n",
      "(5, 'q2_freq', 0.098132250846775529)\n",
      "(6, 'q2_hash', 0.048583669897663564)\n",
      "(7, 'centroid_distances', 0.045124750875211561)\n",
      "(8, 'shared_count', 0.034664122735581622)\n",
      "(9, 'min_distance_tokens_duplic_removed', 0.015634648844591224)\n",
      "(10, 'max_distance_tokens', 0.015235057110383391)\n",
      "(11, 'common_unigrams_lens', 0.0083551546945300151)\n",
      "(12, 'common_bigrams_ratios', 0.0080283676887583158)\n",
      "(13, 'common_bigrams_lens', 0.0065986723259292045)\n",
      "(14, 'common_trigrams_ratios', 0.0047232347146133231)\n",
      "(15, 'avg_world_len2', 0.0046620571114406482)\n",
      "(16, 'q1_hash', 0.0045524519793498043)\n",
      "(17, 'min_distances', 0.0040307482243674246)\n",
      "(18, 'len_char_q2', 0.0038471040855632631)\n",
      "(19, 'len_char_q1', 0.0036583403885415556)\n",
      "(20, 'common_trigrams_lens', 0.0035828412313121862)\n",
      "(21, 'diff_stops_r', 0.0034408959519328995)\n",
      "(22, 'diff_avg_word', 0.0033799645200448796)\n",
      "(23, 'avg_world_len1', 0.0032635849906731077)\n",
      "(24, 'stops1_ratio', 0.0032523427539185301)\n",
      "(25, 'diff_len_char', 0.0028396674895080488)\n",
      "(26, 'stops2_ratio', 0.0026038381720036433)\n",
      "(27, 'lens2', 0.0025100417402485671)\n",
      "(28, 'len_word_q1', 0.0019553746595327372)\n",
      "(29, 'diff_len_word', 0.0017983926097268094)\n",
      "(30, 'lens1', 0.0017804208226817259)\n",
      "(31, 'len_word_q2', 0.0016162009832423455)\n",
      "(32, 'dif_len', 0.0)\n",
      "(33, 'exactly_same', 0.0)\n",
      "(34, 'duplicated', 0.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALICAYAAABiqwZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu4LHdZJ/rvGzaogCiIZgtIoiKojIqoMWdE08goARUY\nnUfBowgzRznzCHJmRgUdj4meGQXH8fYwjoMi43XiERTwjhyzvIwiQcJFSUgUiAlI5OYg4gXDe/7o\n3tm/vdKr16Vr7dV77c/nefpZ1auq3vpVd1Wt76r+dVV1dwAAgLkLjroBAACwSQRkAAAYCMgAADAQ\nkAEAYCAgAwDAQEAGAICBgAywgarqv1bVvz/qdgCcj8p1kIHjpKrenOSjkvxjkkrSSR7Y3W9bo+Zl\nSX66uz9mkkaeY6rq+Ulu7u5vP+q2AJwNJ466AQAT6yRf1N1XT1jzVNA+2MxVd+ru2yZsz1lTVT5p\nBM47DnzAcVRLf1l1aVX9z6p6d1VduzgzfGrck6rq9VX1nqr606r6usXv75rkV5Pcp6r+ejH+ZFU9\nv6q+c5j/sqq6eXj+pqr65qp6TZL3VtUFVfXRVfWCqvrLqvqzqnrajisw1D9Vu6q+aTHvW6rqcVX1\nqKq6oareUVXPHOa9oqp+vqquWrT3lVX1qcP4T6yqqxevw+uq6ku2LfeHq+pXquqvk/yrJP97km9e\n1HrxYrpnLF6n91TVH1fV44YaX1NVv1tV/6mq3rVY18uH8fesqh9frMc7q+oXhnFfvHhv3l1Vv1dV\nnzKMe0ZV3bJY5nVV9fCdXj+AdQjIwHmhqu6T5JeTfGd33zPJNyZ5YVV9xGKSW5M8urvvkeTJSb6/\nqh7S3e9L8qgkb+3uD+3ue6zorrH9LPPjF/N++GLcLyW5NslHJ3lEkqdX1RfscRVOJrnLYt4rkvxo\nkq9K8pAkn5fk26vqomH6xyT5uST3TPI/kryoqu5UVScW7fj1JB+Z5BuS/ExVfcIw7xOS/D/d/aFJ\nfjLJzyT5nsW6P3YxzZ8m+ZzF6/UdSX66qi4calyS5LokH5HkPyV53jDup5N8SJJPyrw7zPcnSVV9\n+mK6r01yryT/LclLqurOVfXAJF+f5DMWy3xkkjfv8bUD2BcBGTiOXrQ4c/mu4ezkVyX5le7+jSTp\n7v8vySuTPHrx/Ne6+82L4d9N8tIkn7tmO36wu9/a3X+f5LOS3Lu7/2N337ZY1o9lHqL34h+SfNei\nq8ZVmQfP7+/u93X365O8PsmnDdP/UXf/4mL670vyQUkuXTzu1t3P7u5/XHRF+eXMQ/EpL+7ulyfJ\nou130N0v7O5bF8M/n+TGzEPxKTd194/3/IsuP5Hko6vqo6rqZObh9ind/Z7Fa/G7i3m+NsmPdPcr\ne+6nkvz9os23Zf4Pwj+pqhPd/efd/aY9vnYA+6IPMnAcPXZJH+SLknz50J2gMj8G/laSVNWjknx7\nkgdmfvLgQ5K8ds123LJt+fetqncNy78gye/ssdY7+/S3qv928fMvh/F/m+Tuw/Pbu3t0d1fVW5Lc\nZ7Hcm3Omm5Lcd9m8O6mqJyb5N0kuXvzqbknuPUxy+1n27v7bqsqifR+R5F3d/Z4lZS9K8sSh60kl\nuXOS+3T371bV/5XkyiSfXFW/keTfdfdf7NZWgP0SkIHjaFkf5JuT/GR3P+UOE1fdJckLMj/L/OLu\n/kBV/eJQZ9kX9P4myV2H5x+9ZJpxvpuTvLG7H7SH9k/h9itu1Dyd3i/JWzNfp/tvm/b+Sd4wPN++\nvmc8r6r7J3lukod39x8sfndtduj7vc3NSe5VVfdYEpJvTvIfu/u7l83Y3Vcluaqq7r5Y/rOSfM0e\nlgmwL7pYAOeLn07yJVX1hYsvzH3w4stv98n8o/u7JHnHIhw/KskXDvPemuQjquoew+9eneTRiy+c\nnUzy9F2W/4okf7344t4HL/oDP7iqPnO6VTzDZyy+yHenzM/0/l2Slyf5wyR/s2jHiaqaJfnizPsp\n7+TWJB83PL9bkg8kecfitXxykn+yl0Yt+m//WpIfrqoPX7ThVFeWH03yf1bVJUlSVXerqkcvfj6w\nqh6++GfmHzI/Y/6BPb0SAPskIAPHzdLLsXX3LUkem+Rbk7w9824F35jkgu5+b+ZfVvv5RReIxyd5\n8TDvGzIPkG9c9Gs+meSnMu+C8ebMv/B21ap2dPcHMg+iD0nypsy7R/xoknvkYFae5V20/yuSvDvz\nq1D880V/3/cn+ZLM+16/I8lzknx1d9+4Q51k/sW5B5/q093d12Xer/nlmXeleHCS39tHe7868+tU\nX595+H56knT3H2XeD/k5i/fhhpw+Q/xBmZ8xfnvmZ8I/Msm37LJMgAOZ5EYhi8v3/EDmgft53f3s\nJdPMMv+m8p2TvL27XZ4H4BBU1RVJPr67n3jUbQE4F63dB7nmF5F/TuaXLHprkmuq6sXdff0wzYcl\n+S9JvrC731JV915eDQAAjtYUXSwuSXJjd9+0+Ojuqsw/xhx9ZZIXdvdbkqS73zHBcgEAYHJTXMXi\nvjnzkkC35MxrYSbzyybduaquzvwyPz+0uL4lABPr7u846jYAnMvO1mXeTiR5aJLPz/zbz39QVX/Q\n3X+6fcKqWr9TNAAA7KK7l16ecoouFm/JmdfUvN/id6NbkvxGd/9dd78z8wvjf1p20N2TPK644gq1\n1Dpn2qaW7UKt41trk9umlu3ifK21yhQB+ZokD6iqixbXp3x8kpdsm+bFSR62uO7nXZN8dpLrJlg2\nAABMau0uFt19W1U9NclLc/oyb9dV1VPmo/u53X394ragr01yW5Lndvfr1102AABMbZI+yN3960ke\ntO13/23b8+9N8r1TLG+vZrOZWmodaj21jketqeuppdZh11PreNSaup5a05nkRiFTqqretDYBAHC8\nVFX6EL+kBwAAx4aADAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAAD\nARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgI\nyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBA\nBgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIy\nAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJAB\nAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwA\nAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAxOHHUD\npra1NX+cGp7N5sOz2elhAADYSXX3UbfhDFXVU7WpKtmw1QMAYANUVbq7lo3TxQIAAAYCMgAADARk\nAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAINJAnJVXV5V11fVDVX1jCXjL6uqv6qqVy0e3zbF\ncgEAYGpr32q6qi5I8pwkj0jy1iTXVNWLu/v6bZP+Tnc/Zt3lAQDAYZriDPIlSW7s7pu6+/1Jrkry\n2CXTLb2VHwAAbJIpAvJ9k9w8PL9l8bvt/reqenVV/UpVffIEywUAgMmt3cVij/4oyf27+31V9agk\nL0rywJ0mvvLKK28fns1mmc1mh90+AACOsa2trWxtbe1p2urutRZWVZcmubK7L188f2aS7u5nr5jn\nTUk+o7vftWRcr9um07WSiUoBAHCMVFW6e2kX4Cm6WFyT5AFVdVFV3SXJ45O8ZFsDLhyGL8k8mN8h\nHAMAwFFbu4tFd99WVU9N8tLMA/fzuvu6qnrKfHQ/N8m/qKp/neT9Sf42yVesu1wAADgMa3exmJou\nFgAAHLbD7mIBAADHhoAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwA\nAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAA\nGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDA\nQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAG\nAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQ\nkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGA\nDAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARk\nAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgMElA\nrqrLq+r6qrqhqp6xYrrPqqr3V9WXTrFcAACY2toBuaouSPKcJI9M8uAkT6iqT9xhumcl+Y11lwkA\nAIdlijPIlyS5sbtv6u73J7kqyWOXTPe0JC9I8pcTLBMAAA7FFAH5vkluHp7fsvjd7arqPkke193/\nNUlNsEwAADgUJ87Scn4gydg3eWVIvvLKK28fns1mmc1mh9IoAADOD1tbW9na2trTtNXday2sqi5N\ncmV3X754/swk3d3PHqZ546nBJPdO8jdJvq67X7KkXq/bptO1kolKAQBwjFRVunvpSdspAvKdkrwh\nySOS/EWSVyR5Qndft8P0z0/yS939CzuMF5ABADhUqwLy2l0suvu2qnpqkpdm3qf5ed19XVU9ZT66\nn7t9lnWXCQAAh2XtM8hTcwYZAIDDtuoMsjvpAQDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgI\nyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBA\nBgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIy\nAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJAB\nAGBw4qgbsMm2tuaPU8Oz2Xx4Njs9DADA8VLdfdRtOENV9VRtqkqmWr0pawEAcLSqKt1dy8bpYgEA\nAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAA\nGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDA\nQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAG\nAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYTBKQq+ry\nqrq+qm6oqmcsGf+YqnpNVV1bVa+sqs+fYrkAADC16u71ClRdkOSGJI9I8tYk1yR5fHdfP0xz1+5+\n32L4U5L8Ync/YId6vW6bTtdKJio1aa11bW3NH6eGZ7P58Gx2ehgAgJ1VVbq7lo07MUH9S5Lc2N03\nLRZ2VZLHJrk9IJ8Kxwt3T/KOCZZ73hqDcNXpsAwAwPqm6GJx3yQ3D89vWfzuDFX1uKq6LsmvJvmG\nCZYLAACTm+IM8p5094uSvKiqHpbkp5I8aKdpr7zyytuHZ7NZZvoNAACwhq2trWzt8WP3KfogX5rk\nyu6+fPH8mUm6u5+9Yp4/S3JJd79zyTh9kPdhU9sFALDJVvVBnqKLxTVJHlBVF1XVXZI8PslLtjXg\n44fhhybJsnAMAABHbe0uFt19W1U9NclLMw/cz+vu66rqKfPR/dwkX1ZVT0zyD0n+JslXrLtcAAA4\nDGt3sZiaLhb7s6ntAgDYZIfdxQIAAI4NARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAG\nAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIA\nAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAbndEC++OTJVNWOjyQ7jrv45Mkj\nbj0AAJuouvuo23CGquq9tqmqsmrKSqdTO4xL9rPuVcmGvVRJNrddAACbrKrS3UuD4jl9BhkAAKYm\nIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAAMB\nGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAAGAjI\nAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAG\nAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIA\nAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEA\nYCAgAwDAQEAGAIDBJAG5qi6vquur6oaqesaS8V9ZVa9ZPH6vqj5liuUCAMDU1g7IVXVBkuckeWSS\nByd5QlV94rbJ3pjk87r705L8hyQ/uu5yAQDgMExxBvmSJDd2903d/f4kVyV57DhBd7+8u//X4unL\nk9x3guUCAMDkpgjI901y8/D8lqwOwP9Hkl+bYLkAADC5E2dzYVX18CRPTvKwVdNdeeWVtw/PZrPM\nZrNDbRcAAMfb1tZWtra29jRtdfdaC6uqS5Nc2d2XL54/M0l397O3TfepSV6Y5PLu/rMV9Xqvbaqq\nrJqy0unUDuOS/ax7VbLmS3UoNrVdAACbrKrS3UuD4hRdLK5J8oCquqiq7pLk8Ulesq0B9888HH/1\nqnAMAABHbe0uFt19W1U9NclLMw/cz+vu66rqKfPR/dwk/3eSeyX54aqqJO/v7kvWXTYAAExt7S4W\nU9PFYn82tV0AAJvssLtYAADAsSEgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQ\nAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAM\nAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABieOugHni62t\n+ePU8Gw2H57NTg8DAHD0qruPug1nqKrea5uqKqumrHQ6tcO4ZD/rXpVM9VJtai0AgPNFVaW7lwZF\nXSwAAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICB\ngAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAAAwEZAAAGAvLCxSdP\npqp2fCTZcdzFJ08ecesBAJhKdfdRt+EMVdV7bVNVZdWUlU6ndhiXjMuZstZuqpKpXvYpawEAnC+q\nKt29NNw5gwwAAIMTR90AjtbW1vxxang2mw/PZqeHAQDOJ7pYHEKt3WxqFwvdNQCA84UuFgAAsEe6\nWByCi0+ezE233rpiir79yhjLXHThhXnz2942fcPOIbp+AABHRReLDau1rN4q50MXi01tFwBw7tLF\nAgAA9khABgCAgYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEAYCAgAwDAQEAGAICBgAwAAIMT\nR90AVrv45MncdOutK6boVC29jXiS5KILL8yb3/a26Rt2jtnamj9ODc9m8+HZ7PQwAECSVHcfdRvO\nUFW91zZVVVZNWel0lofHSjIuZ1Nqba83Za3dVCUbtjkkmb5dm7qeAMDZU1Xp7qUhShcLAAAYCMgA\nADAQkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYA\ngMEkAbmqLq+q66vqhqp6xpLxD6qq36+qv6uqfzvFMgEA4DBUd69XoOqCJDckeUSStya5Jsnju/v6\nYZp7J7koyeOSvLu7v29Fvd5rm6oqq6asdDq1w7hkXM5h19rKZdnKbDE8yyxbSZJZtjLLb+9Yb512\nLWvbKlXJmpvDoZi6XZu6ngDA2VNV6e6lIerEBPUvSXJjd9+0WNhVSR6b5PaA3N3vSPKOqvriCZZ3\nTprlt4cg/B1H2hYAAHY2RReL+ya5eXh+y+J3AABwzpniDPLkrrzyytuHZ7NZZrPZkbUFAIBz39bW\nVra2tvY07RR9kC9NcmV3X754/swk3d3PXjLtFUn++nzsg7wf+iCfSR9kAGBqq/ogT9HF4pokD6iq\ni6rqLkken+Qlq9ozwTI5oItPnkxVLX0k2XFcVeXikyePuPUAAIdv7TPIyfwyb0l+MPPA/bzuflZV\nPSXzM8nPraoLk7wyyYcm+UCS9yb55O5+75JaziDn8M4gr6o35dnoKTmDDABMbdUZ5EkC8pQEZAH5\nDssVkAGAiR12FwsAADg2BGQAABgIyAAAMBCQAQBgICADAMBgI++kx7lpa2v+ODV86gaIs9npYQCA\nTecyb0dQazfH4TJvU15KzWXeAICpucwbAADskYAMAAADARkAAAYCMgAADARkAAAYCMgAADAQkAEA\nYCAgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYAgIGADAAA\nAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADKq7j7oN\nZ6iq3mubqiqrpqx0OrXDuGRcztmstZux3jrt2kvbtnJZtjJbDM8yy1aSZJatzPLbK2utXIdKptq0\npqx1GPUAgHNPVaW7l4YoAfkIau3mbAbkg7Zr12kFZABgg60KyLpYAADAQEAGAICBgAwAAAMBGQAA\nBgIyAAAMBGQAABgIyAAAMBCQAQBgICBzYBefPJmqWvpIsuO4qsrFJ08ecesBAJZzJ70jqLWbc+VO\neqtq7bddK5frTnoAwMTcSQ8AAPZIQGYj6K4BAGwKXSyOoNZuzscuFmeru8Z8Wcezi8XW1vxxang2\nmw/PZqeHAYC5VV0sBOQjqLUbAXm9Wrs5rgF5dD6sIwCsQx9kAADYIwEZAAAGAjIAAAwEZAAAGAjI\nAAAwEJABAGAgIAMAwEBA5thZdVe+3e7M5658AIAbhRxBrd24Ucjh1dqt3n5vOrKp3CgEAFZzoxAA\nANijE0fdAGDzbG3NH6eGZ7P58Gx2ehgAjitdLI6g1m50sTi8WrvV08XicGsBwKbQxQIAAPZIFwvY\nJ90PAOB408XiCGrtRheLw6u1W739drFYt/vBYYVtXSwAYLVVXSwE5COotRsB+fBq7VbvbAfk860W\nAGwKfZABAGCPBGQAABgIyAAAMBCQAQBgICADAMDg2F3FYiuXZSuzxfAss2wlSWbZyiy/fXreuIrF\nOu3arZarWOzf+VALADbFeXuZt5XzRkBep1271RKQ9+98qAUAm8Jl3gAAYI8EZAAAGAjIsMLFJ0+m\nqnZ8JNlx3MUnTx5x6wGAg9AHeY+19EE+P/sgT1nr4pMnc9Ott66o1ou5lrvowgvz5re9bcX8w7I3\nqA/y1tb8cWp4NpsPz2anhwHgbPMlvWXzRkBep1271RKQp621rN4qmxSQD6sWAKzDl/QAAGCPThx1\nAwD2SncNAM4GXSz2WEsXC10sjrLWsnqrbGq3iE2tBcD5Z1UXC2eQAYCN4FMiNoUzyHus5QyyM8hH\nWWt7vSmviHG+Xl2DoyUIsRv7OIfNVSyWzRsBeZ12Lau1lcuyldlieJZZtpIks2xllt8+cLsE5DvW\n29RauznOAVngO7hNey/ZDMd5u3C82AwC8rJ5IyCv066zWUtAvmO9Ta2V7HZGerPORh/WH6lN+sN+\nLvwh3qTXi81xvmwX58t6biIBedm8EZDXadfZrCUg37Heptbard6mno2eut6m3lzluLaL4+l82S7O\nl/XcRIcekKvq8iQ/kPl1lZ/X3c9eMs0PJXlUkr9J8qTufvUOtQTkCMhnTisgb6+3qbV2q3eUZ6PX\n6Wt9PtRaVm+V4/oPxdT/BGxq23yysDnOl/XcRIcakKvqgiQ3JHlEkrcmuSbJ47v7+mGaRyV5and/\nUVV9dpIf7O5Ld6gnIEdAPnPa4xuQD9pv+3wJyIdda6+v/7m8je211vZ6U4fts/fPzqbWWl1vU2st\nq7fdpn6ycK78QyEgH53DDsiXJrmiux+1eP7MJD2eRa6qH0lydXf/3OL5dUlm3X2HPVZAFpDvOO1m\nBeTDClW7EZAPr9Zujss2tu52salfxJ2yXVOv45Tv5VHU2l5vk6+gcz7847TMufBpwKY67ID8ZUke\n2d1ft3j+VUku6e5vGKb5pSTf3d2/v3j+siTf3N2vWlJPQI6AfOa0mxWQ9+pc2S4E5N1t6j9OR/kp\nxRRtOz3tZr2Xau1cb1M/CVtWb5P/cdrrem6vddhh+3yzKiCnu9d6JPmyJM8dnn9Vkh/aNs0vJfmn\nw/OXJXnoDvX6iiuuuP1x9dVX904uuvDCznxr2Pfjogsv3GetnrDW3tu2TrumbtvR1Zp6Hc/Oe3mu\nbBdndxvb1Fqbuu1v8jY2Zdu8l5ta6zC3C3/fdl/PvdW6rJMrFo+rh+HLVta64oorltR6+qLG9sfT\nz5juiiuumKzW9npT1tru6quvPiNjJuneId9O1cXiyu6+fPF8L10srk9yWa/ZxeJs2tQ+Qpvarilt\n8tUKNtVVbWw7AAAO/0lEQVQmfYHqXKjF0fJessz5sl2cL+u5iQ67i8Wdkrwh8y/p/UWSVyR5Qndf\nN0zz6CRf3/Mv6V2a5Ad6gi/pnU2bugFvarumJCDv3yYF0XPlizKcXd5LdnM+HKuT82c9N9HZuszb\nD+b0Zd6eVVVPyfxM8nMX0zwnyeWZX+btyb2k//FiOgF5Hza1XVMSkPdvkwIywEGcL8ee82U9N9Gh\nB+QpCcj7s6ntmpKAvH8CMnAuOl8+WThf1nPTCcgT2NSQsKntmpKAvH8CMgCstiogX3C2GwMAAJtM\nQAYAgIGADAAAAwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGDgRiF7tKk3S9jUdk3JjUL2b911dJcn\nAI47d9KbwKaGqk1t15QE5P07H9YRANbhTnoAALBHAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBA\nBgCAgYAMAAADARkAAAYCMgAADARkAAAYVHcfdRvOUFW9aW1KkqpkA5u1se2a0tTreFxfs62t+ePU\n8Gw2H57NTg8DAHNVle6upeM2LYwKyPuzqe2akoAMAExtVUDWxQIAAAYCMgAADARkAAAYCMgAADAQ\nkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgUN191G04Q1X1prUp\nSaqSDWzWxrZrXVtb88ep4dlsPjybnR4+qOP6mgEAe1dV6e5aOm7TwqiAvD+b2q5N5jUDAFYFZF0s\nAABgICADAMBAQAYAgIGADAAAAwEZAAAGrmKxR5t65YNNbdemOczLxgEA5x6XeZvApgbRTW0XAMAm\nc5k3AADYIwEZAAAGAjIAAAwEZAAAGAjIAAAwEJABAGAgIAMAwEBABgCAgYAMAAADARkAAAZuNb1H\nm3RL562t+ePU8Gw2H57NTg8DALCzVbeaFpD3aJMCMgAA61kVkHWxAACAgYAMAAADARkAAAYCMgAA\nDARkAAAYCMgAADBwmbcVXG8YAOB4ch1kAAAYuA4yAADskYAMAAADARkAAAYCMgAADARkAAAYCMgA\nADAQkAEAYCAgAwDAQEAGAICBgAwAAAMBGQAABgIyAAAMBGQAABgIyAAAMBCQAQBgICADAMBAQAYA\ngIGADAAAAwEZAAAGAjIAAAwEZAAAGKwVkKvqnlX10qp6Q1X9RlV92A7TPa+qbq2q166zvP3a2tpS\nS61DrafW8ag1dT211Drsemodj1pT11NrOuueQX5mkpd194OS/FaSb9lhuucneeSay9q3TX1D1Dq6\nWlPXU+t41Jq6nlpqHXY9tY5HranrqTWddQPyY5P8xGL4J5I8btlE3f17Sd695rIAAODQrRuQP6q7\nb02S7n5bko9av0kAAHB0qrtXT1D1m0kuHH+VpJN8W5L/3t33GqZ9Z3d/xA51LkryS939qbssb3WD\nAABgAt1dy35/Yg8zfsFO4xZfvLuwu2+tqpNJ/nKNNp5a3tKGAgDA2bBuF4uXJHnSYvhrkrx4xbS1\neAAAwMZaNyA/O8kXVNUbkjwiybOSpKo+uqp++dREVfWzSX4/yQOr6s+r6slrLhcAAA7Frn2QAQDg\nfHIs7qS37EYkVfVZVfWKqrp28fMz16x3VVW9avF4U1W9ao1a/6Kq/riqbquqh+61XbvVXXf+qvqe\nqrquql5dVS+sqnusUWtPN5HZY63vrKrXLNr1sqq63xq1PrWqfn9R78VVdfe91FpR74qqumXYNi4/\naK3F75+2eA9eV1XP2mvbhvkvr6rrq+qGqnrGfuffVuuDquoPF/vQn1TVd61R635V9VuLOq+rqm9Y\ns21vXryH11bVK/Y579L1Oug2u6320xfrd6B1XLVfV9W/q6oPVNW9ls27bdqVr/eatZ62+P2B9ssl\n9S9Y7DsvOcC8O7Vt38eyFdvFvo/XO73+VfVp+z3+rKi17+11xeu17+PYinZN8fftgYv34VWLn/9r\nP/vTqu2/9nmMXbGel9QBM8ZQ+8Oq6ucX7fmTqvrsNdbx1Ht5oKyyrfa3LOq+tqp+pqruso95d9qP\nptwnJzn27Kq7z/lHkocleUiS1w6/uzrJFy6GH5Xk6nXqbRv/vUm+bY22PSjJJ2R+c5WHTrXOE7xm\n/yzJBYvhZyX57jVqPTvJNy+Gn5HkWWvUuvsw/LQkP7ZGrVckedhi+ElJvnPN1+yKJP92otd/luSl\nSU4snt97nzUvSPKnSS5Kcuckr07yiQfZPoaad138vFOSlyf5nAPWOZnkIafezyRvWKdtSd6Y5J5T\nrtdBt9mh5oOTvDbJBy3qvjTJx627XSx+f78kv57kTUnutc7rPVWtg+6XS+r/myQ/neQlU21XOfix\nbNl2se/j9ZJ2XZ/kk3KA48+Kddz39rqi1r6PYytqrf33bdtyLkjy1iQfM0HbZtnnMXbFe3ngjDHU\n/u9JnrwYPpHkHuuu47Zp9pxVhnkuyvz4epfF859L8sR91li2H025T05y7NntcSzOIPfyG5H8RZJT\n/1F/eJK3rFlv9OVJ/sdBa3X3G7r7xqzxpcU9tPEg7XpZd39g8fTlmf8hPWhb9nQTmT22673D07sl\necca7fqExe+T5GVJvmwvtVbUSw7wPu5Q619n/kfuHxfT7Gk9B5ckubG7b+ru9ye5KvP34cC6+32L\nwQ/K/A/Vgba57n5bd796MfzeJNclue8aTaus8QnYDut1oG128ElJ/rC7/767b0vyO0m+dJ/t2mkb\n+/4k37SPOqte70lqHXS/HC3O/Dw6yY/td95d2nbQY9kdtouDHK+XtOv6zF//fR9/dljH++UA2+su\n28W+jmMrXvu1/75t88+S/Fl337xu23KAY+wO7+V9Ms8YH76YbF8ZI0kWZ1A/t7ufv6j9j939nr3O\nv8dj6p6zyuA9Sf4hyd2q6kSSu2b+D8qe7bAfTblPrn3s2YtjEZB38Mwk31dVf57ke7LzbbD3pao+\nN8nbuvvPpqi3wf5lkl9bY/5JbyJTVf9h8V4+Kcl3r1HqT6rqMYvhL88ed9JdPHXxUc+P7eVjzhUe\nmOTzqurlVXX1AT6yu2+S8Y/ILVkvhJ76+PvaJG9LstXdr1+n3qLmxZmfJf3DNcp0kt+sqmuq6msP\n0IZl63XhmtvsHyf53MVH33fNPPh9zH7btqStj0lyc3e/7oDzX5zF6z1lrcXzdffLU2F97S/DrNiu\n9nwsO+Tt/eVZ8/izrdZa2+uS1+vAx7GJ9umdfEX2H/Jut61tax1jt9V6ZpL/vEbG+Ngk76iq5y+6\nQzy3qj5knzWWtevU7w6UVbr73Un+c5I/zzz0/1V3v2yf7dltP1p7n5wwE+zoOAfk5yV5WnffP/OP\n8H58orpPyBo767mgqv59kvd3989OWHatP4Dd/W2L9/L5SX5gjVL/MsnXV9U1mf/n+Q/rtCvJD2f+\nMfpDMt+Bv2+NWicy7zZwaZJvTvL/rtm2tXX3B7r70zP/Q/55VXXZOvVq3ufyBUmevu0swH59Tnc/\nNPMQ+vVV9bD9zLxtvT63qma54za6r222u6/P/GPv30zyq0muTXLbfmpst/iD+a2ZfwR++6/3Mf/t\nr/eiLZPUOvXerbNfVtUXJbl1cRZsrcuA7rRd7fdYdha293+VAx5/ltQ68Pa6pNaBj2MT7tPLat85\nyWOS/PwB59/etgMfY5fUWjdjnEjy0CT/ZXEse1/moXtfVrz+B8oqVfVxma/PRZmfKb97VX3lfmqs\n2o+m2icnzAQ7Os4B+bO7+0VJ0t0vyPzj57VU1Z0y/8j059attamq6kmZh4597RBL3FpVFy5qTnIT\nmYWfTbLvL0Oc0t03dPcju/uzMu+CsNYnAd399u4+9YfpR5N81hrlbk7yC4u61yT5QFUtvTPlDt6S\n5P7D8/tlnx/77WTx0d+vZI3XfvFx3QuS/FR3r7pm+l7a8xeLn29P8os54P69WK9fzXy91t5mu/v5\n3f2Z3T1L8ldJbjhIuwYfn+TiJK+pqjdl/p7+UVXterZwyes9Za3tDrJffk6Sx1TVGzP/Q/7wqvrJ\nfdbYsW3rHMsOa3tfdD/Y9/Fnh3U80Pa6Q7sOdBybcp/ewaOS/NFiP9+XHdp2oGPsDrXWzRi3ZP5p\nzisXz1+QeWDesxXb/jpZ5TOT/M/uflfPu4r9QpJ/eoA6d9iPDmmfXCsTrHKcAvL2MxA3nvpPo6oe\nkf3/oVp2RuMLklzX3fvqj7NDrXHcQa1785Uz5q/5N5e/Kcljuvvv12zLfm4is1u7HjCMe1zmXz47\naK2PXPy8IPPbpf/IPmotq3dyGPelmX/MfqBaSV6U5PMXdR+Y5M7d/c591LsmyQOq6qKaf+v48Zm/\nDwdSVfc+9VHr4kzmF2R/r/12P57k9d39g2vUSFXddXHWJFV1tyRfmH287jus17VZb5s9VfvU9nX/\nJP8884P3vsssHunuP+7uk939cd39sZn/Uf307t5LGDrj9Z6yVrL2fpnu/tbuvn93f1zm2+pvdfcT\n91NjRdv2fSzb4/a+n+PtsnYd9PizbN856Pa6rF0HPY7ttk+v2w95nU9sl7XtoMfYZbXWyhiL7jE3\nL9qRzO8lsd8uPTu9/gfNKsn8y36XVtUHV1Ut2nXdXmfeaT+acp9c99izZ30I3/w724/M/wi9Ncnf\nZ95v5slJPiPz/jjXJvmDzP8QHLje4vfPT/J1E7TtcZn/J/u3mXf0/7Up1nmCdt2Y5KYkr1o8fniN\nWvfM/Esob8j8W8MfvkatFyR53eK9fGHm/ZsPWusbFm26Psl3TfCa/WTmVy54deYH3wvXqHUiyU8t\n1vWVSS47wHZx+WL9bkzyzDX3q09ZbAfXJnlNkm9co9bnZP4R/6sX9V6V5PID1vrYoc7r9rueO61X\nknsdZJvdVvt3Mg8X1yaZHWD+lft15t8u38uVJ3Z9vdetddD9codlXJaDXcViWdselQMcy1ZsF/s+\nXq94zfZ9/FlRa9/b64pa+z6Orai19t+3Rf27Jnl7kg+daLu4PPOr++zrGLui1oEzxlD70zI/sfHq\nzM/Ufti667gYt++ssq32NyX5k8U28ROZ/yOx13l32o+m3CdfsGjb2seeVQ83CgEAgMFx6mIBAABr\nE5ABAGAgIAMAwEBABgCAgYAMAAADARkAAAYCMgAADP5/WVEFXtpS0VcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15e1b3908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "forest = clf\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "names = ['common_unigrams_lens','common_unigrams_ratios','common_bigrams_lens','common_bigrams_ratios',\n",
    "         'common_trigrams_lens','common_trigrams_ratios','lens1','lens2','dif_len','q1_hash','q2_hash',\n",
    "         'q1_freq','q2_freq','centroid_distances','min_distances','min_distance_tokens_duplic_removed',\n",
    "         'max_distance_tokens']+ list(x.columns)\n",
    "for f in range(X.shape[1]):\n",
    "    print((f + 1, names[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=9, max_features=6, max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "Testing_Set = pd.read_csv('test.csv')\n",
    "print(Testing_Set.shape)\n",
    "clean_data_test = Testing_Set[['question1','question2']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic_features = select_basic_features_test(clean_data_test)\n",
    "new_features_test = test_comb[['q1_hash','q2_hash','q1_freq','q2_freq']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(basic_features.shape,new_features_test.shape)\n",
    "X_submission = np.column_stack([basic_features,new_features_test])\n",
    "X_submission = preprocessing.scale(X_submission)\n",
    "y_submission = clf.predict_proba(X_submission)[:,1]\n",
    "Testing_Set['is_duplicate'] = y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = Testing_Set[['test_id','is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = submission.to_csv('/Users/pascalsitbon/work/Kaggle/pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
