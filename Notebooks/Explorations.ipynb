{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm, preprocessing, cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.learning_curve import learning_curve\n",
    "import random\n",
    "import numpy as np\n",
    "import gensim\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import nltk\n",
    "import itertools\n",
    "import glove\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize, ngrams\n",
    "import re\n",
    "from gensim import corpora\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pascalsitbon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Loading Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training_Set = pd.read_csv('train.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404288, 6)\n"
     ]
    }
   ],
   "source": [
    "print(Training_Set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method to find separation of slits using fresnel biprism?\n"
     ]
    }
   ],
   "source": [
    "print(Training_Set.iloc[10,:]['question1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = Training_Set[['question1','question2','is_duplicate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_train = clean_data[:,:2]\n",
    "labels = clean_data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of data \n",
      " [['What is the step by step guide to invest in share market in india?'\n",
      "  'What is the step by step guide to invest in share market?']\n",
      " ['What is the story of Kohinoor (Koh-i-Noor) Diamond?'\n",
      "  'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?']\n",
      " [ 'How can I increase the speed of my internet connection while using a VPN?'\n",
      "  'How can Internet speed be increased by hacking through DNS?']\n",
      " ['Why am I mentally very lonely? How can I solve it?'\n",
      "  'Find the remainder when [math]23^{24}[/math] is divided by 24,23?']\n",
      " [ 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?'\n",
      "  'Which fish would survive in salt water?']\n",
      " [ 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?'\n",
      "  \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"]\n",
      " ['Should I buy tiago?'\n",
      "  'What keeps childern active and far from phone and video games?']\n",
      " ['How can I be a good geologist?'\n",
      "  'What should I do to be a great geologist?']\n",
      " ['When do you use シ instead of し?' 'When do you use \"&\" instead of \"and\"?']\n",
      " ['Motorola (company): Can I hack my Charter Motorolla DCX3400?'\n",
      "  'How do I hack Motorola DCX3400 for free internet?']] [0 0 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('sample of data','\\n',sentences_train[:10],labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_train = 400000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence_train):\n",
    "    sentences = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(sentence_train.shape[0]): \n",
    "        source_sentence = sentences_train[i,0].lower().split(\" \") \n",
    "        source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "        unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "        sentences.append(unigrams_que1)\n",
    "\n",
    "        target_sentence = sentences_train[i,1].lower().split(\" \") \n",
    "        target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "        unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "        sentences.append(unigrams_que2)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = preprocess(sentences_train[:size_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-15 22:41:13,536 : INFO : collecting all words and their counts\n",
      "2017-05-15 22:41:13,537 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-15 22:41:13,572 : INFO : PROGRESS: at sentence #10000, processed 57070 words, keeping 13175 word types\n",
      "2017-05-15 22:41:13,616 : INFO : PROGRESS: at sentence #20000, processed 113800 words, keeping 20410 word types\n",
      "2017-05-15 22:41:13,672 : INFO : PROGRESS: at sentence #30000, processed 170684 words, keeping 26246 word types\n",
      "2017-05-15 22:41:13,702 : INFO : PROGRESS: at sentence #40000, processed 227355 words, keeping 31320 word types\n",
      "2017-05-15 22:41:13,728 : INFO : PROGRESS: at sentence #50000, processed 284506 words, keeping 36058 word types\n",
      "2017-05-15 22:41:13,758 : INFO : PROGRESS: at sentence #60000, processed 341143 words, keeping 40245 word types\n",
      "2017-05-15 22:41:13,790 : INFO : PROGRESS: at sentence #70000, processed 397752 words, keeping 44200 word types\n",
      "2017-05-15 22:41:13,818 : INFO : PROGRESS: at sentence #80000, processed 454391 words, keeping 47821 word types\n",
      "2017-05-15 22:41:13,847 : INFO : PROGRESS: at sentence #90000, processed 511330 words, keeping 51264 word types\n",
      "2017-05-15 22:41:13,877 : INFO : PROGRESS: at sentence #100000, processed 568451 words, keeping 54700 word types\n",
      "2017-05-15 22:41:13,907 : INFO : PROGRESS: at sentence #110000, processed 625141 words, keeping 57878 word types\n",
      "2017-05-15 22:41:13,935 : INFO : PROGRESS: at sentence #120000, processed 682074 words, keeping 60984 word types\n",
      "2017-05-15 22:41:13,964 : INFO : PROGRESS: at sentence #130000, processed 738507 words, keeping 63887 word types\n",
      "2017-05-15 22:41:13,998 : INFO : PROGRESS: at sentence #140000, processed 795857 words, keeping 66813 word types\n",
      "2017-05-15 22:41:14,026 : INFO : PROGRESS: at sentence #150000, processed 852298 words, keeping 69468 word types\n",
      "2017-05-15 22:41:14,055 : INFO : PROGRESS: at sentence #160000, processed 909999 words, keeping 72112 word types\n",
      "2017-05-15 22:41:14,087 : INFO : PROGRESS: at sentence #170000, processed 966538 words, keeping 74630 word types\n",
      "2017-05-15 22:41:14,119 : INFO : PROGRESS: at sentence #180000, processed 1023360 words, keeping 77124 word types\n",
      "2017-05-15 22:41:14,144 : INFO : PROGRESS: at sentence #190000, processed 1080862 words, keeping 79710 word types\n",
      "2017-05-15 22:41:14,170 : INFO : PROGRESS: at sentence #200000, processed 1136863 words, keeping 82063 word types\n",
      "2017-05-15 22:41:14,206 : INFO : PROGRESS: at sentence #210000, processed 1192872 words, keeping 84437 word types\n",
      "2017-05-15 22:41:14,240 : INFO : PROGRESS: at sentence #220000, processed 1249177 words, keeping 86700 word types\n",
      "2017-05-15 22:41:14,286 : INFO : PROGRESS: at sentence #230000, processed 1305828 words, keeping 88989 word types\n",
      "2017-05-15 22:41:14,315 : INFO : PROGRESS: at sentence #240000, processed 1362644 words, keeping 91160 word types\n",
      "2017-05-15 22:41:14,343 : INFO : PROGRESS: at sentence #250000, processed 1419904 words, keeping 93407 word types\n",
      "2017-05-15 22:41:14,373 : INFO : PROGRESS: at sentence #260000, processed 1476715 words, keeping 95560 word types\n",
      "2017-05-15 22:41:14,402 : INFO : PROGRESS: at sentence #270000, processed 1533932 words, keeping 97711 word types\n",
      "2017-05-15 22:41:14,434 : INFO : PROGRESS: at sentence #280000, processed 1590946 words, keeping 99767 word types\n",
      "2017-05-15 22:41:14,465 : INFO : PROGRESS: at sentence #290000, processed 1648287 words, keeping 101954 word types\n",
      "2017-05-15 22:41:14,496 : INFO : PROGRESS: at sentence #300000, processed 1705301 words, keeping 103960 word types\n",
      "2017-05-15 22:41:14,523 : INFO : PROGRESS: at sentence #310000, processed 1762700 words, keeping 106072 word types\n",
      "2017-05-15 22:41:14,554 : INFO : PROGRESS: at sentence #320000, processed 1819867 words, keeping 108102 word types\n",
      "2017-05-15 22:41:14,582 : INFO : PROGRESS: at sentence #330000, processed 1877140 words, keeping 109947 word types\n",
      "2017-05-15 22:41:14,618 : INFO : PROGRESS: at sentence #340000, processed 1933890 words, keeping 111743 word types\n",
      "2017-05-15 22:41:14,648 : INFO : PROGRESS: at sentence #350000, processed 1991299 words, keeping 113582 word types\n",
      "2017-05-15 22:41:14,677 : INFO : PROGRESS: at sentence #360000, processed 2048434 words, keeping 115450 word types\n",
      "2017-05-15 22:41:14,707 : INFO : PROGRESS: at sentence #370000, processed 2105753 words, keeping 117252 word types\n",
      "2017-05-15 22:41:14,738 : INFO : PROGRESS: at sentence #380000, processed 2162621 words, keeping 119005 word types\n",
      "2017-05-15 22:41:14,768 : INFO : PROGRESS: at sentence #390000, processed 2218869 words, keeping 120692 word types\n",
      "2017-05-15 22:41:14,803 : INFO : PROGRESS: at sentence #400000, processed 2275833 words, keeping 122488 word types\n",
      "2017-05-15 22:41:14,834 : INFO : PROGRESS: at sentence #410000, processed 2332462 words, keeping 124195 word types\n",
      "2017-05-15 22:41:14,865 : INFO : PROGRESS: at sentence #420000, processed 2389052 words, keeping 125888 word types\n",
      "2017-05-15 22:41:14,894 : INFO : PROGRESS: at sentence #430000, processed 2445786 words, keeping 127483 word types\n",
      "2017-05-15 22:41:14,925 : INFO : PROGRESS: at sentence #440000, processed 2502619 words, keeping 129244 word types\n",
      "2017-05-15 22:41:14,955 : INFO : PROGRESS: at sentence #450000, processed 2559732 words, keeping 130952 word types\n",
      "2017-05-15 22:41:14,985 : INFO : PROGRESS: at sentence #460000, processed 2616606 words, keeping 132556 word types\n",
      "2017-05-15 22:41:15,014 : INFO : PROGRESS: at sentence #470000, processed 2672973 words, keeping 134103 word types\n",
      "2017-05-15 22:41:15,043 : INFO : PROGRESS: at sentence #480000, processed 2729906 words, keeping 135719 word types\n",
      "2017-05-15 22:41:15,074 : INFO : PROGRESS: at sentence #490000, processed 2787360 words, keeping 137314 word types\n",
      "2017-05-15 22:41:15,101 : INFO : PROGRESS: at sentence #500000, processed 2844145 words, keeping 138928 word types\n",
      "2017-05-15 22:41:15,132 : INFO : PROGRESS: at sentence #510000, processed 2901448 words, keeping 140530 word types\n",
      "2017-05-15 22:41:15,171 : INFO : PROGRESS: at sentence #520000, processed 2958822 words, keeping 142071 word types\n",
      "2017-05-15 22:41:15,211 : INFO : PROGRESS: at sentence #530000, processed 3015975 words, keeping 143664 word types\n",
      "2017-05-15 22:41:15,253 : INFO : PROGRESS: at sentence #540000, processed 3073650 words, keeping 145213 word types\n",
      "2017-05-15 22:41:15,284 : INFO : PROGRESS: at sentence #550000, processed 3130478 words, keeping 146737 word types\n",
      "2017-05-15 22:41:15,317 : INFO : PROGRESS: at sentence #560000, processed 3187420 words, keeping 148271 word types\n",
      "2017-05-15 22:41:15,349 : INFO : PROGRESS: at sentence #570000, processed 3244360 words, keeping 149868 word types\n",
      "2017-05-15 22:41:15,382 : INFO : PROGRESS: at sentence #580000, processed 3301174 words, keeping 151346 word types\n",
      "2017-05-15 22:41:15,411 : INFO : PROGRESS: at sentence #590000, processed 3357764 words, keeping 152841 word types\n",
      "2017-05-15 22:41:15,441 : INFO : PROGRESS: at sentence #600000, processed 3414131 words, keeping 154291 word types\n",
      "2017-05-15 22:41:15,477 : INFO : PROGRESS: at sentence #610000, processed 3470864 words, keeping 155676 word types\n",
      "2017-05-15 22:41:15,513 : INFO : PROGRESS: at sentence #620000, processed 3527758 words, keeping 157128 word types\n",
      "2017-05-15 22:41:15,543 : INFO : PROGRESS: at sentence #630000, processed 3584730 words, keeping 158473 word types\n",
      "2017-05-15 22:41:15,576 : INFO : PROGRESS: at sentence #640000, processed 3640963 words, keeping 159802 word types\n",
      "2017-05-15 22:41:15,602 : INFO : PROGRESS: at sentence #650000, processed 3697916 words, keeping 161310 word types\n",
      "2017-05-15 22:41:15,634 : INFO : PROGRESS: at sentence #660000, processed 3755218 words, keeping 162713 word types\n",
      "2017-05-15 22:41:15,670 : INFO : PROGRESS: at sentence #670000, processed 3812231 words, keeping 164040 word types\n",
      "2017-05-15 22:41:15,699 : INFO : PROGRESS: at sentence #680000, processed 3868508 words, keeping 165270 word types\n",
      "2017-05-15 22:41:15,732 : INFO : PROGRESS: at sentence #690000, processed 3925401 words, keeping 166633 word types\n",
      "2017-05-15 22:41:15,763 : INFO : PROGRESS: at sentence #700000, processed 3982230 words, keeping 167903 word types\n",
      "2017-05-15 22:41:15,793 : INFO : PROGRESS: at sentence #710000, processed 4039303 words, keeping 169230 word types\n",
      "2017-05-15 22:41:15,826 : INFO : PROGRESS: at sentence #720000, processed 4096126 words, keeping 170592 word types\n",
      "2017-05-15 22:41:15,858 : INFO : PROGRESS: at sentence #730000, processed 4153019 words, keeping 171905 word types\n",
      "2017-05-15 22:41:15,888 : INFO : PROGRESS: at sentence #740000, processed 4209580 words, keeping 173191 word types\n",
      "2017-05-15 22:41:15,918 : INFO : PROGRESS: at sentence #750000, processed 4266618 words, keeping 174590 word types\n",
      "2017-05-15 22:41:15,964 : INFO : PROGRESS: at sentence #760000, processed 4324031 words, keeping 175900 word types\n",
      "2017-05-15 22:41:15,996 : INFO : PROGRESS: at sentence #770000, processed 4381118 words, keeping 177217 word types\n",
      "2017-05-15 22:41:16,034 : INFO : PROGRESS: at sentence #780000, processed 4439024 words, keeping 178528 word types\n",
      "2017-05-15 22:41:16,070 : INFO : PROGRESS: at sentence #790000, processed 4496840 words, keeping 179809 word types\n",
      "2017-05-15 22:41:16,101 : INFO : collected 181046 word types from a corpus of 4554164 raw words and 800000 sentences\n",
      "2017-05-15 22:41:16,102 : INFO : Loading a fresh vocabulary\n",
      "2017-05-15 22:41:17,040 : INFO : min_count=1 retains 181046 unique words (100% of original 181046, drops 0)\n",
      "2017-05-15 22:41:17,041 : INFO : min_count=1 leaves 4554164 word corpus (100% of original 4554164, drops 0)\n",
      "2017-05-15 22:41:17,608 : INFO : deleting the raw counts dictionary of 181046 items\n",
      "2017-05-15 22:41:17,618 : INFO : sample=0.001 downsamples 17 most-common words\n",
      "2017-05-15 22:41:17,619 : INFO : downsampling leaves estimated 4401475 word corpus (96.6% of prior 4554164)\n",
      "2017-05-15 22:41:17,620 : INFO : estimated required memory for 181046 words and 100 dimensions: 235359800 bytes\n",
      "2017-05-15 22:41:18,282 : INFO : resetting layer weights\n",
      "2017-05-15 22:41:20,896 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:41:21,915 : INFO : PROGRESS: at 2.42% examples, 530097 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:22,926 : INFO : PROGRESS: at 5.15% examples, 561200 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:41:23,931 : INFO : PROGRESS: at 7.87% examples, 572810 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:24,939 : INFO : PROGRESS: at 10.54% examples, 575702 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:41:25,946 : INFO : PROGRESS: at 13.26% examples, 579643 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:26,964 : INFO : PROGRESS: at 16.03% examples, 582711 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:27,970 : INFO : PROGRESS: at 18.71% examples, 583166 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:28,977 : INFO : PROGRESS: at 21.38% examples, 583350 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:41:29,990 : INFO : PROGRESS: at 24.02% examples, 582120 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:41:30,993 : INFO : PROGRESS: at 26.31% examples, 574034 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:31,994 : INFO : PROGRESS: at 28.28% examples, 561471 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:33,035 : INFO : PROGRESS: at 30.91% examples, 561063 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:41:34,053 : INFO : PROGRESS: at 33.67% examples, 563913 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-15 22:41:35,054 : INFO : PROGRESS: at 36.45% examples, 567038 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:41:36,077 : INFO : PROGRESS: at 39.08% examples, 567050 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:41:37,102 : INFO : PROGRESS: at 41.93% examples, 569930 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:41:38,134 : INFO : PROGRESS: at 44.17% examples, 564390 words/s, in_qsize 6, out_qsize 3\n",
      "2017-05-15 22:41:39,185 : INFO : PROGRESS: at 46.11% examples, 555207 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:41:40,204 : INFO : PROGRESS: at 48.30% examples, 550905 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:41:41,243 : INFO : PROGRESS: at 50.85% examples, 550330 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:42,268 : INFO : PROGRESS: at 53.91% examples, 555584 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:43,299 : INFO : PROGRESS: at 56.95% examples, 559776 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:41:44,315 : INFO : PROGRESS: at 59.93% examples, 563524 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:45,332 : INFO : PROGRESS: at 62.87% examples, 566588 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:41:46,341 : INFO : PROGRESS: at 65.87% examples, 569929 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:41:47,352 : INFO : PROGRESS: at 68.88% examples, 573360 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:41:48,354 : INFO : PROGRESS: at 71.83% examples, 576000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:49,354 : INFO : PROGRESS: at 74.72% examples, 578163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:50,359 : INFO : PROGRESS: at 77.76% examples, 581081 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:51,367 : INFO : PROGRESS: at 80.73% examples, 583427 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:52,368 : INFO : PROGRESS: at 83.73% examples, 585745 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:53,378 : INFO : PROGRESS: at 86.76% examples, 588051 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:54,394 : INFO : PROGRESS: at 89.74% examples, 589826 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:55,398 : INFO : PROGRESS: at 92.55% examples, 590589 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:56,408 : INFO : PROGRESS: at 95.58% examples, 592570 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:41:57,432 : INFO : PROGRESS: at 98.62% examples, 594204 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:41:57,847 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:41:57,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:41:57,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:41:57,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:41:57,882 : INFO : training on 22770820 raw words (22007283 effective words) took 37.0s, 595292 effective words/s\n",
      "2017-05-15 22:41:57,883 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:41:58,899 : INFO : PROGRESS: at 2.82% examples, 616795 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:41:59,909 : INFO : PROGRESS: at 5.81% examples, 633631 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:00,913 : INFO : PROGRESS: at 8.74% examples, 637480 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:42:01,914 : INFO : PROGRESS: at 11.73% examples, 642163 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:02,919 : INFO : PROGRESS: at 14.75% examples, 646371 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:03,939 : INFO : PROGRESS: at 17.79% examples, 647588 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:04,963 : INFO : PROGRESS: at 20.81% examples, 648118 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:05,990 : INFO : PROGRESS: at 23.76% examples, 645885 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:07,014 : INFO : PROGRESS: at 26.88% examples, 648528 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:08,022 : INFO : PROGRESS: at 29.86% examples, 648838 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:09,042 : INFO : PROGRESS: at 32.93% examples, 650152 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:10,044 : INFO : PROGRESS: at 35.87% examples, 649820 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:11,052 : INFO : PROGRESS: at 38.86% examples, 649968 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:12,067 : INFO : PROGRESS: at 41.89% examples, 650458 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:13,070 : INFO : PROGRESS: at 44.92% examples, 651399 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:14,083 : INFO : PROGRESS: at 47.95% examples, 651801 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:15,102 : INFO : PROGRESS: at 50.93% examples, 651393 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:16,130 : INFO : PROGRESS: at 53.96% examples, 651255 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:17,147 : INFO : PROGRESS: at 57.04% examples, 651967 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:18,172 : INFO : PROGRESS: at 60.10% examples, 652359 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:19,185 : INFO : PROGRESS: at 63.18% examples, 653108 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:20,190 : INFO : PROGRESS: at 66.22% examples, 653538 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:21,194 : INFO : PROGRESS: at 69.23% examples, 654008 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:22,196 : INFO : PROGRESS: at 72.18% examples, 653671 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:23,214 : INFO : PROGRESS: at 75.21% examples, 653701 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:24,220 : INFO : PROGRESS: at 78.33% examples, 654798 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:25,221 : INFO : PROGRESS: at 81.31% examples, 654865 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:26,258 : INFO : PROGRESS: at 84.30% examples, 654070 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:27,284 : INFO : PROGRESS: at 87.46% examples, 654913 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:28,318 : INFO : PROGRESS: at 90.49% examples, 654575 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:29,318 : INFO : PROGRESS: at 93.21% examples, 652797 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:30,323 : INFO : PROGRESS: at 96.15% examples, 652526 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:31,329 : INFO : PROGRESS: at 98.96% examples, 651389 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:31,630 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:42:31,632 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:42:31,647 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:42:31,650 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:42:31,651 : INFO : training on 22770820 raw words (22007187 effective words) took 33.8s, 651993 effective words/s\n",
      "2017-05-15 22:42:31,652 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:42:32,693 : INFO : PROGRESS: at 2.90% examples, 621245 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:33,695 : INFO : PROGRESS: at 5.94% examples, 642860 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:34,709 : INFO : PROGRESS: at 8.92% examples, 644647 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:35,731 : INFO : PROGRESS: at 11.91% examples, 644300 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:36,741 : INFO : PROGRESS: at 14.93% examples, 647412 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:37,766 : INFO : PROGRESS: at 17.97% examples, 647889 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:38,769 : INFO : PROGRESS: at 21.03% examples, 651625 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:39,783 : INFO : PROGRESS: at 24.06% examples, 652347 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:40,795 : INFO : PROGRESS: at 27.10% examples, 653040 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:41,806 : INFO : PROGRESS: at 30.17% examples, 654601 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:42,828 : INFO : PROGRESS: at 33.28% examples, 656152 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:43,828 : INFO : PROGRESS: at 36.23% examples, 655376 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:44,830 : INFO : PROGRESS: at 39.17% examples, 654637 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:45,844 : INFO : PROGRESS: at 42.19% examples, 654866 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:46,868 : INFO : PROGRESS: at 45.23% examples, 654615 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:47,881 : INFO : PROGRESS: at 48.21% examples, 654243 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:48,887 : INFO : PROGRESS: at 51.24% examples, 654759 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:42:49,890 : INFO : PROGRESS: at 54.22% examples, 654772 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:50,890 : INFO : PROGRESS: at 57.17% examples, 654382 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:51,896 : INFO : PROGRESS: at 60.19% examples, 654804 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:52,903 : INFO : PROGRESS: at 63.27% examples, 655614 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:53,910 : INFO : PROGRESS: at 66.26% examples, 655439 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:54,914 : INFO : PROGRESS: at 69.19% examples, 654985 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:55,928 : INFO : PROGRESS: at 72.14% examples, 654293 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:56,939 : INFO : PROGRESS: at 75.12% examples, 654113 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:57,943 : INFO : PROGRESS: at 78.16% examples, 654483 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:42:58,951 : INFO : PROGRESS: at 81.13% examples, 654390 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:42:59,961 : INFO : PROGRESS: at 84.16% examples, 654597 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:00,987 : INFO : PROGRESS: at 87.20% examples, 654424 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:01,990 : INFO : PROGRESS: at 90.13% examples, 654112 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:02,996 : INFO : PROGRESS: at 93.16% examples, 654387 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:04,000 : INFO : PROGRESS: at 96.15% examples, 654395 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:05,009 : INFO : PROGRESS: at 99.14% examples, 654288 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:05,238 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:43:05,243 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:43:05,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:43:05,263 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:43:05,264 : INFO : training on 22770820 raw words (22006444 effective words) took 33.6s, 655026 effective words/s\n",
      "2017-05-15 22:43:05,265 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:43:06,295 : INFO : PROGRESS: at 2.90% examples, 629739 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:07,308 : INFO : PROGRESS: at 5.81% examples, 629650 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:08,323 : INFO : PROGRESS: at 8.70% examples, 629219 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:09,324 : INFO : PROGRESS: at 11.60% examples, 631091 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:10,363 : INFO : PROGRESS: at 14.53% examples, 629420 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-15 22:43:11,365 : INFO : PROGRESS: at 17.48% examples, 632116 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:12,369 : INFO : PROGRESS: at 20.46% examples, 635283 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:13,369 : INFO : PROGRESS: at 23.49% examples, 639144 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:14,386 : INFO : PROGRESS: at 26.53% examples, 640895 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:15,393 : INFO : PROGRESS: at 29.50% examples, 642049 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:16,421 : INFO : PROGRESS: at 32.54% examples, 642685 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:17,438 : INFO : PROGRESS: at 35.52% examples, 642911 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:18,444 : INFO : PROGRESS: at 38.51% examples, 643687 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:19,460 : INFO : PROGRESS: at 41.58% examples, 645260 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:20,501 : INFO : PROGRESS: at 44.70% examples, 646223 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:21,526 : INFO : PROGRESS: at 47.86% examples, 648265 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:22,528 : INFO : PROGRESS: at 50.76% examples, 647571 words/s, in_qsize 8, out_qsize 2\n",
      "2017-05-15 22:43:23,550 : INFO : PROGRESS: at 53.87% examples, 648885 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:24,555 : INFO : PROGRESS: at 56.86% examples, 649150 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:25,568 : INFO : PROGRESS: at 59.84% examples, 649112 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:26,586 : INFO : PROGRESS: at 62.78% examples, 648487 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:27,599 : INFO : PROGRESS: at 65.78% examples, 648472 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:28,620 : INFO : PROGRESS: at 68.88% examples, 649512 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:29,632 : INFO : PROGRESS: at 71.83% examples, 649088 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:30,644 : INFO : PROGRESS: at 74.81% examples, 649095 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:31,651 : INFO : PROGRESS: at 77.80% examples, 649211 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:32,659 : INFO : PROGRESS: at 80.78% examples, 649325 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:33,664 : INFO : PROGRESS: at 83.77% examples, 649482 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:34,664 : INFO : PROGRESS: at 86.76% examples, 649727 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:35,687 : INFO : PROGRESS: at 89.74% examples, 649467 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:36,688 : INFO : PROGRESS: at 92.73% examples, 649691 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:37,691 : INFO : PROGRESS: at 95.67% examples, 649570 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:38,711 : INFO : PROGRESS: at 98.83% examples, 650559 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:39,049 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:43:39,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:43:39,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:43:39,070 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:43:39,071 : INFO : training on 22770820 raw words (22005991 effective words) took 33.8s, 651301 effective words/s\n",
      "2017-05-15 22:43:39,072 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:43:40,089 : INFO : PROGRESS: at 2.82% examples, 615669 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:41,095 : INFO : PROGRESS: at 5.90% examples, 644124 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:42,116 : INFO : PROGRESS: at 8.92% examples, 647012 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:43,135 : INFO : PROGRESS: at 11.95% examples, 648836 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:44,167 : INFO : PROGRESS: at 14.93% examples, 646323 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:45,176 : INFO : PROGRESS: at 17.97% examples, 648674 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:46,186 : INFO : PROGRESS: at 20.90% examples, 647589 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:47,199 : INFO : PROGRESS: at 23.89% examples, 647764 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:48,204 : INFO : PROGRESS: at 26.93% examples, 649445 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:49,220 : INFO : PROGRESS: at 29.95% examples, 650116 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:50,225 : INFO : PROGRESS: at 32.89% examples, 649587 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:43:51,227 : INFO : PROGRESS: at 35.92% examples, 650873 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:52,237 : INFO : PROGRESS: at 38.91% examples, 650840 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:53,252 : INFO : PROGRESS: at 42.02% examples, 652601 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:43:54,268 : INFO : PROGRESS: at 45.10% examples, 653478 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:55,283 : INFO : PROGRESS: at 48.08% examples, 653090 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:56,296 : INFO : PROGRESS: at 51.02% examples, 652258 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:57,310 : INFO : PROGRESS: at 54.13% examples, 653621 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:43:58,310 : INFO : PROGRESS: at 57.21% examples, 654792 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:43:59,324 : INFO : PROGRESS: at 60.28% examples, 655428 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:00,335 : INFO : PROGRESS: at 63.35% examples, 656062 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:01,366 : INFO : PROGRESS: at 66.35% examples, 655190 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:02,378 : INFO : PROGRESS: at 69.37% examples, 655349 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:03,379 : INFO : PROGRESS: at 72.45% examples, 656176 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:44:04,410 : INFO : PROGRESS: at 75.52% examples, 656158 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:05,414 : INFO : PROGRESS: at 78.51% examples, 656093 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:06,431 : INFO : PROGRESS: at 81.48% examples, 655723 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:07,450 : INFO : PROGRESS: at 84.52% examples, 655675 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:08,461 : INFO : PROGRESS: at 87.55% examples, 655817 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:09,473 : INFO : PROGRESS: at 90.49% examples, 655289 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:10,482 : INFO : PROGRESS: at 93.47% examples, 655154 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:44:11,483 : INFO : PROGRESS: at 96.55% examples, 655772 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:12,484 : INFO : PROGRESS: at 99.57% examples, 656093 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:12,576 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:44:12,586 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:44:12,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:44:12,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:44:12,599 : INFO : training on 22770820 raw words (22007316 effective words) took 33.5s, 656669 effective words/s\n",
      "2017-05-15 22:44:12,600 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:44:13,631 : INFO : PROGRESS: at 2.90% examples, 627293 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:14,650 : INFO : PROGRESS: at 5.94% examples, 640538 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:15,673 : INFO : PROGRESS: at 8.96% examples, 644371 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:16,677 : INFO : PROGRESS: at 11.91% examples, 644282 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:17,682 : INFO : PROGRESS: at 14.76% examples, 640538 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:18,713 : INFO : PROGRESS: at 17.79% examples, 641541 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:44:19,724 : INFO : PROGRESS: at 20.90% examples, 646795 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:44:20,725 : INFO : PROGRESS: at 23.97% examples, 650368 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:21,731 : INFO : PROGRESS: at 26.93% examples, 649591 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:22,740 : INFO : PROGRESS: at 29.95% examples, 650702 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:23,754 : INFO : PROGRESS: at 32.93% examples, 650400 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:24,767 : INFO : PROGRESS: at 35.97% examples, 651013 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:25,796 : INFO : PROGRESS: at 38.95% examples, 650032 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:26,797 : INFO : PROGRESS: at 41.97% examples, 651151 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:27,815 : INFO : PROGRESS: at 45.01% examples, 651434 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:44:28,829 : INFO : PROGRESS: at 48.04% examples, 651797 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:29,834 : INFO : PROGRESS: at 51.11% examples, 653010 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:30,854 : INFO : PROGRESS: at 54.04% examples, 652003 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:31,870 : INFO : PROGRESS: at 57.04% examples, 651716 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:32,875 : INFO : PROGRESS: at 60.06% examples, 652319 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:33,886 : INFO : PROGRESS: at 63.09% examples, 652643 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:44:34,924 : INFO : PROGRESS: at 66.17% examples, 652591 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:44:35,926 : INFO : PROGRESS: at 69.19% examples, 653146 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:36,950 : INFO : PROGRESS: at 72.23% examples, 653052 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:37,952 : INFO : PROGRESS: at 75.25% examples, 653547 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:38,964 : INFO : PROGRESS: at 78.33% examples, 654100 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:39,965 : INFO : PROGRESS: at 81.31% examples, 654199 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:40,969 : INFO : PROGRESS: at 84.34% examples, 654533 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:41,982 : INFO : PROGRESS: at 87.37% examples, 654668 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:42,990 : INFO : PROGRESS: at 90.40% examples, 654886 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:44,013 : INFO : PROGRESS: at 93.42% examples, 654781 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:45,029 : INFO : PROGRESS: at 96.50% examples, 655114 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:46,050 : INFO : PROGRESS: at 99.44% examples, 654471 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:46,179 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:44:46,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:44:46,200 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:44:46,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:44:46,203 : INFO : training on 22770820 raw words (22006544 effective words) took 33.6s, 655180 effective words/s\n",
      "2017-05-15 22:44:46,204 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:44:47,224 : INFO : PROGRESS: at 2.82% examples, 618002 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:48,232 : INFO : PROGRESS: at 5.81% examples, 634735 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:49,245 : INFO : PROGRESS: at 8.83% examples, 642463 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:50,245 : INFO : PROGRESS: at 11.82% examples, 646083 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:51,268 : INFO : PROGRESS: at 14.89% examples, 649199 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:52,271 : INFO : PROGRESS: at 17.92% examples, 651725 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:53,287 : INFO : PROGRESS: at 20.94% examples, 652334 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:54,295 : INFO : PROGRESS: at 23.93% examples, 652302 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:55,296 : INFO : PROGRESS: at 26.93% examples, 652690 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:44:56,308 : INFO : PROGRESS: at 29.81% examples, 650417 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:44:57,323 : INFO : PROGRESS: at 32.85% examples, 651050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:58,331 : INFO : PROGRESS: at 35.83% examples, 651058 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:44:59,349 : INFO : PROGRESS: at 38.86% examples, 651367 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:45:00,353 : INFO : PROGRESS: at 41.89% examples, 652273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:01,354 : INFO : PROGRESS: at 44.87% examples, 652510 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:02,374 : INFO : PROGRESS: at 47.91% examples, 652615 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:03,402 : INFO : PROGRESS: at 50.89% examples, 651777 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:04,416 : INFO : PROGRESS: at 53.96% examples, 652641 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:45:05,434 : INFO : PROGRESS: at 56.95% examples, 652247 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:06,440 : INFO : PROGRESS: at 59.93% examples, 652302 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:07,471 : INFO : PROGRESS: at 62.96% examples, 652037 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:08,492 : INFO : PROGRESS: at 66.04% examples, 652502 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:09,502 : INFO : PROGRESS: at 68.97% examples, 652010 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:10,538 : INFO : PROGRESS: at 72.05% examples, 652057 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-15 22:45:11,556 : INFO : PROGRESS: at 74.99% examples, 651402 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:12,558 : INFO : PROGRESS: at 77.89% examples, 650840 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:13,561 : INFO : PROGRESS: at 80.82% examples, 650633 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:14,567 : INFO : PROGRESS: at 83.68% examples, 649709 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:15,572 : INFO : PROGRESS: at 86.76% examples, 650498 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:45:16,575 : INFO : PROGRESS: at 89.70% examples, 650330 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:17,590 : INFO : PROGRESS: at 92.68% examples, 650229 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:45:18,608 : INFO : PROGRESS: at 95.67% examples, 650092 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:19,608 : INFO : PROGRESS: at 98.75% examples, 650868 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:19,987 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:45:19,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:45:20,000 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:45:20,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:45:20,009 : INFO : training on 22770820 raw words (22007751 effective words) took 33.8s, 651388 effective words/s\n",
      "2017-05-15 22:45:20,010 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:45:21,055 : INFO : PROGRESS: at 2.95% examples, 627166 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:22,067 : INFO : PROGRESS: at 6.03% examples, 647528 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:23,077 : INFO : PROGRESS: at 9.05% examples, 651650 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:24,086 : INFO : PROGRESS: at 12.08% examples, 653960 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:25,102 : INFO : PROGRESS: at 15.07% examples, 652463 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:26,115 : INFO : PROGRESS: at 18.10% examples, 653399 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:45:27,119 : INFO : PROGRESS: at 21.03% examples, 652159 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:28,152 : INFO : PROGRESS: at 24.11% examples, 652465 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:29,166 : INFO : PROGRESS: at 27.23% examples, 655138 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:30,169 : INFO : PROGRESS: at 30.25% examples, 656069 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:31,171 : INFO : PROGRESS: at 33.19% examples, 655154 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:32,172 : INFO : PROGRESS: at 36.14% examples, 654487 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:33,181 : INFO : PROGRESS: at 39.08% examples, 653448 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:34,183 : INFO : PROGRESS: at 42.11% examples, 654313 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:45:35,202 : INFO : PROGRESS: at 45.14% examples, 654322 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:36,220 : INFO : PROGRESS: at 48.25% examples, 655557 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:37,221 : INFO : PROGRESS: at 51.28% examples, 656146 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:38,227 : INFO : PROGRESS: at 54.26% examples, 655986 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:39,233 : INFO : PROGRESS: at 57.22% examples, 655331 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:40,234 : INFO : PROGRESS: at 60.23% examples, 655846 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:41,243 : INFO : PROGRESS: at 63.27% examples, 656087 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:42,262 : INFO : PROGRESS: at 66.30% examples, 655973 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:43,266 : INFO : PROGRESS: at 69.28% examples, 655921 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:44,271 : INFO : PROGRESS: at 72.23% examples, 655439 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:45,294 : INFO : PROGRESS: at 75.30% examples, 655656 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:46,316 : INFO : PROGRESS: at 78.33% examples, 655537 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:47,327 : INFO : PROGRESS: at 81.39% examples, 656033 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:48,331 : INFO : PROGRESS: at 84.38% examples, 655969 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:49,360 : INFO : PROGRESS: at 87.42% examples, 655689 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:45:50,398 : INFO : PROGRESS: at 90.53% examples, 655864 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:45:51,414 : INFO : PROGRESS: at 93.60% examples, 656183 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:52,421 : INFO : PROGRESS: at 96.64% examples, 656365 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:53,423 : INFO : PROGRESS: at 99.66% examples, 656629 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:53,490 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:45:53,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:45:53,512 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:45:53,513 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:45:53,514 : INFO : training on 22770820 raw words (22006602 effective words) took 33.5s, 657114 effective words/s\n",
      "2017-05-15 22:45:53,514 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:45:54,537 : INFO : PROGRESS: at 2.86% examples, 622463 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:55,544 : INFO : PROGRESS: at 5.90% examples, 642176 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:45:56,553 : INFO : PROGRESS: at 8.96% examples, 651464 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:57,555 : INFO : PROGRESS: at 11.91% examples, 650137 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:45:58,576 : INFO : PROGRESS: at 14.93% examples, 650708 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:45:59,580 : INFO : PROGRESS: at 17.92% examples, 651305 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:00,581 : INFO : PROGRESS: at 20.90% examples, 652064 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:01,581 : INFO : PROGRESS: at 23.84% examples, 651506 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:02,581 : INFO : PROGRESS: at 26.79% examples, 651004 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:03,596 : INFO : PROGRESS: at 29.86% examples, 652583 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:04,596 : INFO : PROGRESS: at 32.89% examples, 653836 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:46:05,606 : INFO : PROGRESS: at 35.92% examples, 654339 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:06,610 : INFO : PROGRESS: at 38.99% examples, 655828 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:07,641 : INFO : PROGRESS: at 42.02% examples, 655140 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:08,651 : INFO : PROGRESS: at 45.05% examples, 655468 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:09,663 : INFO : PROGRESS: at 48.12% examples, 656289 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:10,669 : INFO : PROGRESS: at 51.15% examples, 656667 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:11,671 : INFO : PROGRESS: at 54.09% examples, 656073 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:12,672 : INFO : PROGRESS: at 56.33% examples, 647497 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:13,678 : INFO : PROGRESS: at 58.23% examples, 635794 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:14,704 : INFO : PROGRESS: at 60.36% examples, 627347 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:15,722 : INFO : PROGRESS: at 62.39% examples, 618605 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:16,732 : INFO : PROGRESS: at 64.45% examples, 611233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:17,744 : INFO : PROGRESS: at 66.31% examples, 602440 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:18,750 : INFO : PROGRESS: at 68.62% examples, 598723 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:19,758 : INFO : PROGRESS: at 71.26% examples, 597796 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:20,773 : INFO : PROGRESS: at 73.54% examples, 593980 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:21,773 : INFO : PROGRESS: at 76.40% examples, 595182 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:22,796 : INFO : PROGRESS: at 79.03% examples, 594190 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:23,825 : INFO : PROGRESS: at 81.70% examples, 593474 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:24,842 : INFO : PROGRESS: at 84.47% examples, 593627 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:46:25,852 : INFO : PROGRESS: at 87.24% examples, 593927 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:26,855 : INFO : PROGRESS: at 90.27% examples, 596050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:27,864 : INFO : PROGRESS: at 93.03% examples, 596272 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:28,865 : INFO : PROGRESS: at 95.62% examples, 595499 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:29,874 : INFO : PROGRESS: at 98.40% examples, 595719 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:30,508 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:46:30,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:46:30,529 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:46:30,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:46:30,533 : INFO : training on 22770820 raw words (22007370 effective words) took 37.0s, 594737 effective words/s\n",
      "2017-05-15 22:46:30,534 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:46:31,582 : INFO : PROGRESS: at 1.27% examples, 271774 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:32,600 : INFO : PROGRESS: at 2.95% examples, 316060 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:33,615 : INFO : PROGRESS: at 5.68% examples, 406802 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:34,617 : INFO : PROGRESS: at 7.74% examples, 418148 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:35,621 : INFO : PROGRESS: at 9.93% examples, 430666 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:36,635 : INFO : PROGRESS: at 12.04% examples, 435168 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:37,654 : INFO : PROGRESS: at 14.67% examples, 454343 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:38,719 : INFO : PROGRESS: at 17.05% examples, 459008 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:39,790 : INFO : PROGRESS: at 19.28% examples, 459126 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:40,793 : INFO : PROGRESS: at 21.78% examples, 467939 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:41,813 : INFO : PROGRESS: at 24.41% examples, 477047 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:42,825 : INFO : PROGRESS: at 26.84% examples, 480999 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-15 22:46:43,832 : INFO : PROGRESS: at 28.76% examples, 476563 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:44,874 : INFO : PROGRESS: at 31.00% examples, 476266 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:45,888 : INFO : PROGRESS: at 33.11% examples, 475040 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:46,915 : INFO : PROGRESS: at 36.18% examples, 486530 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:47,921 : INFO : PROGRESS: at 38.56% examples, 488382 words/s, in_qsize 7, out_qsize 2\n",
      "2017-05-15 22:46:48,933 : INFO : PROGRESS: at 40.78% examples, 488309 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:49,938 : INFO : PROGRESS: at 43.38% examples, 492414 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:46:50,939 : INFO : PROGRESS: at 45.98% examples, 496179 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:46:51,951 : INFO : PROGRESS: at 48.34% examples, 497099 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:52,965 : INFO : PROGRESS: at 50.54% examples, 496180 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-15 22:46:53,997 : INFO : PROGRESS: at 53.26% examples, 499886 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:55,018 : INFO : PROGRESS: at 55.63% examples, 500328 words/s, in_qsize 4, out_qsize 3\n",
      "2017-05-15 22:46:56,037 : INFO : PROGRESS: at 58.00% examples, 500799 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:57,040 : INFO : PROGRESS: at 60.23% examples, 500444 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:46:58,067 : INFO : PROGRESS: at 62.52% examples, 500020 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:46:59,087 : INFO : PROGRESS: at 64.89% examples, 500428 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:47:00,107 : INFO : PROGRESS: at 67.31% examples, 501139 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:47:01,131 : INFO : PROGRESS: at 69.68% examples, 501421 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:02,156 : INFO : PROGRESS: at 71.97% examples, 501058 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:03,161 : INFO : PROGRESS: at 74.37% examples, 501897 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:04,171 : INFO : PROGRESS: at 76.61% examples, 501477 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:05,179 : INFO : PROGRESS: at 78.95% examples, 501656 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:47:06,181 : INFO : PROGRESS: at 81.31% examples, 502195 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:07,198 : INFO : PROGRESS: at 83.68% examples, 502504 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:08,204 : INFO : PROGRESS: at 86.06% examples, 502934 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:09,230 : INFO : PROGRESS: at 88.38% examples, 502834 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:10,230 : INFO : PROGRESS: at 90.53% examples, 502083 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:11,237 : INFO : PROGRESS: at 92.81% examples, 502005 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:12,248 : INFO : PROGRESS: at 95.27% examples, 502816 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:13,256 : INFO : PROGRESS: at 97.52% examples, 502480 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:14,225 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:47:14,234 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:47:14,251 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:47:14,257 : INFO : PROGRESS: at 100.00% examples, 503553 words/s, in_qsize 0, out_qsize 1\n",
      "2017-05-15 22:47:14,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:47:14,258 : INFO : training on 22770820 raw words (22007637 effective words) took 43.7s, 503536 effective words/s\n",
      "2017-05-15 22:47:14,259 : INFO : training model with 4 workers on 181046 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-15 22:47:15,294 : INFO : PROGRESS: at 1.89% examples, 408253 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:16,313 : INFO : PROGRESS: at 4.22% examples, 455629 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:17,339 : INFO : PROGRESS: at 6.60% examples, 473230 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:18,348 : INFO : PROGRESS: at 9.00% examples, 486403 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:19,388 : INFO : PROGRESS: at 11.33% examples, 487591 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:20,403 : INFO : PROGRESS: at 13.57% examples, 487277 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:21,417 : INFO : PROGRESS: at 15.90% examples, 489732 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:47:22,447 : INFO : PROGRESS: at 18.28% examples, 491897 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:23,466 : INFO : PROGRESS: at 20.41% examples, 488843 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:24,466 : INFO : PROGRESS: at 22.44% examples, 484494 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:25,471 : INFO : PROGRESS: at 24.68% examples, 485047 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:26,494 : INFO : PROGRESS: at 26.88% examples, 483952 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:47:27,517 : INFO : PROGRESS: at 29.24% examples, 485969 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:28,537 : INFO : PROGRESS: at 31.57% examples, 487128 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:29,538 : INFO : PROGRESS: at 33.85% examples, 488102 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:30,542 : INFO : PROGRESS: at 36.05% examples, 487649 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:31,559 : INFO : PROGRESS: at 38.21% examples, 486347 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:32,565 : INFO : PROGRESS: at 40.43% examples, 486554 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:33,573 : INFO : PROGRESS: at 42.72% examples, 487181 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:34,610 : INFO : PROGRESS: at 45.05% examples, 487513 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:35,619 : INFO : PROGRESS: at 47.12% examples, 485727 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-15 22:47:36,624 : INFO : PROGRESS: at 49.26% examples, 485084 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:37,651 : INFO : PROGRESS: at 51.55% examples, 485262 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:38,656 : INFO : PROGRESS: at 53.74% examples, 485064 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:39,666 : INFO : PROGRESS: at 55.72% examples, 482896 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:40,679 : INFO : PROGRESS: at 58.01% examples, 483400 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:41,693 : INFO : PROGRESS: at 60.32% examples, 484202 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:42,708 : INFO : PROGRESS: at 62.30% examples, 482207 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:43,721 : INFO : PROGRESS: at 64.67% examples, 483342 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:44,729 : INFO : PROGRESS: at 66.79% examples, 482563 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-15 22:47:45,733 : INFO : PROGRESS: at 69.23% examples, 484359 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:46,788 : INFO : PROGRESS: at 71.39% examples, 483203 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:47,789 : INFO : PROGRESS: at 73.58% examples, 483187 words/s, in_qsize 6, out_qsize 0\n",
      "2017-05-15 22:47:48,798 : INFO : PROGRESS: at 75.82% examples, 483332 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:49,814 : INFO : PROGRESS: at 78.29% examples, 484746 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:50,816 : INFO : PROGRESS: at 80.52% examples, 484939 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:51,839 : INFO : PROGRESS: at 82.98% examples, 486144 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:52,847 : INFO : PROGRESS: at 85.53% examples, 487964 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-15 22:47:53,851 : INFO : PROGRESS: at 87.99% examples, 489263 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:54,860 : INFO : PROGRESS: at 90.49% examples, 490660 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:55,878 : INFO : PROGRESS: at 92.99% examples, 491900 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:56,888 : INFO : PROGRESS: at 95.41% examples, 492716 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:57,903 : INFO : PROGRESS: at 97.91% examples, 493865 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-15 22:47:58,843 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-15 22:47:58,846 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-15 22:47:58,866 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-15 22:47:58,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-15 22:47:58,870 : INFO : training on 22770820 raw words (22007556 effective words) took 44.6s, 493523 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1,size=100,workers=4)\n",
    "for i in range(100):\n",
    "    model.train(sentences,total_examples=model.corpus_count,epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2vec_features(model,sentences: np.array):\n",
    "    'sentences :  [[token1,...,tokend],..,[token1,...,tokend]]'\n",
    "    'model : Trained Word 2 Vec model'\n",
    "    \n",
    "    max_distance_tokens = []\n",
    "    min_distance_tokens_duplic_removed = []\n",
    "    centroid_distances = []\n",
    "\n",
    "\n",
    "    for i in range(int(len(sentences)/2)):\n",
    "        \n",
    "        set1 = set(sentences[2*i])\n",
    "        set2 = set(sentences[2*i+1])\n",
    "        sym_dif = set1.symmetric_difference(set2)\n",
    "        d_min = 100\n",
    "        d_max = 0\n",
    "        for token1 in set1&sym_dif:\n",
    "            for token2 in set2&sym_dif:\n",
    "                distance_tokens = np.linalg.norm(model[token1] - model[token2])\n",
    "                if distance_tokens <= d_min:\n",
    "                    d_min = distance_tokens\n",
    "                if distance_tokens >= d_max:\n",
    "                    d_max =distance_tokens\n",
    "        \n",
    "        \n",
    "        if min(len(set1),len(set2))>0:\n",
    "            centroid1 = np.sum([model[token1] for token1 in set1],axis=0)/len(set1)\n",
    "            centroid2 = np.sum([model[token2] for token2 in set2],axis=0)/len(set2)\n",
    "            distance_centroid = np.linalg.norm(centroid1-centroid2)\n",
    "        else:\n",
    "            distance_centroid = 100\n",
    "    \n",
    "        max_distance_tokens.append(d_max)\n",
    "        min_distance_tokens_duplic_removed.append(d_min)\n",
    "        centroid_distances.append(distance_centroid)\n",
    "    \n",
    "    \n",
    "    word2vec_features = np.array([centroid_distances,\n",
    "                                 min_distance_tokens_duplic_removed,\n",
    "                                 max_distance_tokens]).T\n",
    "                                            \n",
    "    return word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_features = word2vec_features(model,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.45287886  22.44971824  18.08345711]\n"
     ]
    }
   ],
   "source": [
    "print(sum(word2vec_features/word2vec_features.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.93500876  14.46680641  15.6884346 ]\n",
      " [  6.15852451  11.59368896  22.61016655]\n",
      " [  4.41799212   9.71231079  18.93532753]\n",
      " [ 11.52210903  10.28302288  28.35285568]\n",
      " [  7.43974447  10.41463375  21.26323891]\n",
      " [  3.57115579   1.57695639  23.0575695 ]\n",
      " [  9.53270149   2.34914875  22.75592613]\n",
      " [  5.01376677  10.02753448  10.02753448]\n",
      " [  1.77693188   3.18630266   4.37867069]\n",
      " [  5.70936871   1.92508137  23.66041946]] [0 0 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_features[:10,:],labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def doc2vecs_features(sentences_train,nb_epochs=100,alpha=0.025,min_alpha=0.025):\n",
    "#     sentences = []\n",
    "#     stemmer = PorterStemmer()\n",
    "#     for i in range(sentences_train.shape[0]): \n",
    "#         source_sentence = sentences_train[i,0].lower().split(\" \") \n",
    "#         source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "#         unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "#         sentences.append(unigrams_que1)\n",
    "\n",
    "#         target_sentence = sentences_train[i,1].lower().split(\" \") \n",
    "#         target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "#         unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "#         sentences.append(unigrams_que2)\n",
    "\n",
    "#     texts=sentences.copy()\n",
    "#     documents = []\n",
    "#     ct = 0\n",
    "#     for doc in texts:\n",
    "#         doc = gensim.models.doc2vec.LabeledSentence(words = doc, tags = ['SENT_'+str(ct)])\n",
    "#         ct+=1\n",
    "#         documents.append(doc)\n",
    "#     model = gensim.models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1,workers=4)\n",
    "#     model.build_vocab(documents)\n",
    "\n",
    "#     for epoch in range(100):\n",
    "#         model.train(documents,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        \n",
    "#     most_similar_is_duo_1_2 = []\n",
    "#     most_similar_is_duo_2_1 = []\n",
    "    \n",
    "#     most_similar_score_if_duo_1_2 = []\n",
    "#     most_similar_score_if_duo_2_1 = []\n",
    "    \n",
    "#     n_similarities = []\n",
    "    \n",
    "#     for i in range(sentences_train.shape[0]):\n",
    "        \n",
    "#         most_sim_1 = model.docvecs.most_similar([\"SENT_\"+str(2*i)])[0]\n",
    "#         most_sim_2 = model.docvecs.most_similar([\"SENT_\"+str(2*i+1)])[0]\n",
    "        \n",
    "#         most_similar_is_duo_1_2.append(int(most_sim_1[0] == \"SENT_\"+str(2*i+1)))\n",
    "#         most_similar_is_duo_2_1.append(int(most_sim_2[0] == \"SENT_\"+str(2*i)))\n",
    "        \n",
    "#         most_similar_score_if_duo_1_2.append(int(most_sim_1[0] == \"SENT_\"+str(2*i+1))*most_sim_1[1])\n",
    "#         most_similar_score_if_duo_2_1.append(int(most_sim_2[0] == \"SENT_\"+str(2*i))*most_sim_2[1])\n",
    "        \n",
    "#         n_similarities.append(model.n_similarity(sentences[2*i], sentences[2*i+1]))\n",
    "                                             \n",
    "#         doc_2_vec_features = np.array([n_similarities,\n",
    "#                                        most_similar_score_if_duo_1_2,\n",
    "#                                        most_similar_score_if_duo_2_1,\n",
    "#                                        most_similar_is_duo_1_2,\n",
    "#                                        most_similar_is_duo_2_1])\n",
    "                                            \n",
    "#     return doc_2_vec_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and Hash Tag ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #frquency questions and hash.\n",
    "# train_orig =  pd.read_csv('train.csv', header=0)\n",
    "# test_orig =  pd.read_csv('test.csv', header=0)\n",
    "\n",
    "# tic0=timeit.default_timer()\n",
    "# df1 = train_orig[['question1']].copy()\n",
    "# df2 = train_orig[['question2']].copy()\n",
    "# df1_test = test_orig[['question1']].copy()\n",
    "# df2_test = test_orig[['question2']].copy()\n",
    "\n",
    "# df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "# df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "# train_questions = df1.append(df2)\n",
    "# train_questions = train_questions.append(df1_test)\n",
    "# train_questions = train_questions.append(df2_test)\n",
    "# #train_questions.drop_duplicates(subset = ['qid1'],inplace=True)\n",
    "# train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "# train_questions.reset_index(inplace=True,drop=True)\n",
    "# questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "# train_cp = train_orig.copy()\n",
    "# test_cp = test_orig.copy()\n",
    "# train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "# test_cp['is_duplicate'] = -1\n",
    "# test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "# comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "# comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "# comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "# q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "# q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "# def try_apply_dict(x,dict_to_apply):\n",
    "#     try:\n",
    "#         return dict_to_apply[x]\n",
    "#     except KeyError:\n",
    "#         return 0\n",
    "# #map to frequency space\n",
    "# comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "# comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "# train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "# test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_features(data_set,nb_ex):\n",
    "    # Select basic features\n",
    "    tfidf_cosin_sim = []     \n",
    "    stemmer = PorterStemmer()\n",
    "    dif_len = []\n",
    "    common_unigrams_lens = []\n",
    "    common_unigrams_ratios = []\n",
    "    common_bigrams_lens = []\n",
    "    common_bigrams_ratios = []\n",
    "    common_trigrams_lens = []\n",
    "    common_trigrams_ratios = []\n",
    "    similarities = []\n",
    "    counter = 0\n",
    "    sentences = []\n",
    "    for i in range(nb_ex):\n",
    "        if i%10000== 0:\n",
    "            print(i)\n",
    "        source_sentence = data_set[i,0].lower().split(\" \") \n",
    "        source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "        unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "        sentences.append(unigrams_que1)\n",
    "        \n",
    "        target_sentence = data_set[i,1].lower().split(\" \") \n",
    "        target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "        unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "        sentences.append(unigrams_que2)\n",
    "\n",
    "        \n",
    "        #get unigram features #\n",
    "        common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
    "        common_unigrams_lens.append(common_unigrams_len)\n",
    "        common_unigrams_ratios.append(float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1))\n",
    "        \n",
    "        # get bigram features #\n",
    "        bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n",
    "        bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n",
    "        common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n",
    "        common_bigrams_lens.append(common_bigrams_len)\n",
    "        common_bigrams_ratios.append(float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1))\n",
    "\n",
    "\n",
    "        # get trigram features #\n",
    "        trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n",
    "        trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n",
    "        common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n",
    "        common_trigrams_lens.append(common_trigrams_len)\n",
    "        common_trigrams_ratios.append(float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1))\n",
    "\n",
    "        dif_len.append(abs(len(source_sentence) - len(target_sentence)))\n",
    "    \n",
    "    features = np.array([common_unigrams_lens,\n",
    "                         common_unigrams_ratios,\n",
    "                         common_bigrams_lens,\n",
    "                         common_bigrams_ratios,\n",
    "                         common_trigrams_lens,\n",
    "                         common_trigrams_ratios,\n",
    "                         dif_len]).T\n",
    "    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n"
     ]
    }
   ],
   "source": [
    "n_grams_features,y= select_features(sentences_train,size_train),labels[:size_train].astype(float)#,lsi,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 7)\n"
     ]
    }
   ],
   "source": [
    "print(n_grams_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features = train_comb[['q1_hash','q2_hash','q1_freq','q2_freq']].iloc[:size_train].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(new_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascalsitbon/anaconda/envs/EnvDevMachineLearning/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "X = np.column_stack([n_grams_features,new_features,word2vec_features])\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 14)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,max_depth = 9,max_features=6)\n",
    "clf.fit(X_train,y_train)\n",
    "print('Training Score:',sklearn.metrics.log_loss(y_train,clf.predict_proba(X_train)[:,1]))\n",
    "print('Testing Score:',sklearn.metrics.log_loss(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "Testing_Set = pd.read_csv('test.csv')\n",
    "print(Testing_Set.shape)\n",
    "clean_data_test = Testing_Set[['question1','question2']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic_features = select_basic_features_test(clean_data_test)\n",
    "new_features_test = test_comb[['q1_hash','q2_hash','q1_freq','q2_freq']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(basic_features.shape,new_features_test.shape)\n",
    "X_submission = np.column_stack([basic_features,new_features_test])\n",
    "X_submission = preprocessing.scale(X_submission)\n",
    "y_submission = clf.predict_proba(X_submission)[:,1]\n",
    "Testing_Set['is_duplicate'] = y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = Testing_Set[['test_id','is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = submission.to_csv('/Users/pascalsitbon/work/Kaggle/pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
