{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm, preprocessing, cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.learning_curve import learning_curve\n",
    "import random\n",
    "import numpy as np\n",
    "import gensim\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import nltk\n",
    "import itertools\n",
    "import glove\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize, ngrams\n",
    "import re\n",
    "from gensim import corpora\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pascalsitbon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Loading Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training_Set = pd.read_csv('train.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404288, 6)\n"
     ]
    }
   ],
   "source": [
    "print(Training_Set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method to find separation of slits using fresnel biprism?\n"
     ]
    }
   ],
   "source": [
    "print(Training_Set.iloc[10,:]['question1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = Training_Set[['question1','question2','is_duplicate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_train = clean_data[:,:2]\n",
    "labels = clean_data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of data \n",
      " [['What is the step by step guide to invest in share market in india?'\n",
      "  'What is the step by step guide to invest in share market?']\n",
      " ['What is the story of Kohinoor (Koh-i-Noor) Diamond?'\n",
      "  'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?']\n",
      " [ 'How can I increase the speed of my internet connection while using a VPN?'\n",
      "  'How can Internet speed be increased by hacking through DNS?']\n",
      " ['Why am I mentally very lonely? How can I solve it?'\n",
      "  'Find the remainder when [math]23^{24}[/math] is divided by 24,23?']\n",
      " [ 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?'\n",
      "  'Which fish would survive in salt water?']\n",
      " [ 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?'\n",
      "  \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"]\n",
      " ['Should I buy tiago?'\n",
      "  'What keeps childern active and far from phone and video games?']\n",
      " ['How can I be a good geologist?'\n",
      "  'What should I do to be a great geologist?']\n",
      " ['When do you use シ instead of し?' 'When do you use \"&\" instead of \"and\"?']\n",
      " ['Motorola (company): Can I hack my Charter Motorolla DCX3400?'\n",
      "  'How do I hack Motorola DCX3400 for free internet?']] [0 0 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('sample of data','\\n',sentences_train[:10],labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc2vecs_features(sentences_train,nb_epochs=100,alpha=0.025,min_alpha=0.025):\n",
    "    sentences = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(sentences_train.shape[0]): \n",
    "        source_sentence = sentences_train[i,0].lower().split(\" \") \n",
    "        source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "        unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "        sentences.append(unigrams_que1)\n",
    "\n",
    "        target_sentence = sentences_train[i,1].lower().split(\" \") \n",
    "        target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "        unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "        sentences.append(unigrams_que2)\n",
    "\n",
    "    texts=sentences.copy()\n",
    "    documents = []\n",
    "    ct = 0\n",
    "    for doc in texts:\n",
    "        doc = gensim.models.doc2vec.LabeledSentence(words = doc, tags = ['SENT_'+str(ct)])\n",
    "        ct+=1\n",
    "        documents.append(doc)\n",
    "    model = gensim.models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1,workers=4)\n",
    "    model.build_vocab(documents)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train(documents,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        \n",
    "    most_similar_is_duo_1_2 = []\n",
    "    most_similar_is_duo_2_1 = []\n",
    "    \n",
    "    most_similar_score_if_duo_1_2 = []\n",
    "    most_similar_score_if_duo_2_1 = []\n",
    "    \n",
    "    n_similarities = []\n",
    "    \n",
    "    for i in range(sentences_train.shape[0]):\n",
    "        \n",
    "        most_sim_1 = model.docvecs.most_similar([\"SENT_\"+str(2*i)])[0]\n",
    "        most_sim_2 = model.docvecs.most_similar([\"SENT_\"+str(2*i+1)])[0]\n",
    "        \n",
    "        most_similar_is_duo_1_2.append(int(most_sim_1[0] == \"SENT_\"+str(2*i+1)))\n",
    "        most_similar_is_duo_2_1.append(int(most_sim_2[0] == \"SENT_\"+str(2*i)))\n",
    "        \n",
    "        most_similar_score_if_duo_1_2.append(int(most_sim_1[0] == \"SENT_\"+str(2*i+1))*most_sim_1[1])\n",
    "        most_similar_score_if_duo_2_1.append(int(most_sim_2[0] == \"SENT_\"+str(2*i))*most_sim_2[1])\n",
    "        \n",
    "        n_similarities.append(model.n_similarity(sentences[2*i], sentences[2*i+1]))\n",
    "                                             \n",
    "        doc_2_vec_features = np.array([n_similarities,\n",
    "                                       most_similar_score_if_duo_1_2,\n",
    "                                       most_similar_score_if_duo_2_1,\n",
    "                                       most_similar_is_duo_1_2,\n",
    "                                       most_similar_is_duo_2_1])\n",
    "                                            \n",
    "    return doc_2_vec_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-14 17:15:01,232 : INFO : collecting all words and their counts\n",
      "2017-05-14 17:15:01,233 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-05-14 17:15:01,235 : INFO : collected 87 word types and 20 unique tags from a corpus of 20 examples and 111 words\n",
      "2017-05-14 17:15:01,236 : INFO : Loading a fresh vocabulary\n",
      "2017-05-14 17:15:01,238 : INFO : min_count=1 retains 87 unique words (100% of original 87, drops 0)\n",
      "2017-05-14 17:15:01,239 : INFO : min_count=1 leaves 111 word corpus (100% of original 111, drops 0)\n",
      "2017-05-14 17:15:01,241 : INFO : deleting the raw counts dictionary of 87 items\n",
      "2017-05-14 17:15:01,242 : INFO : sample=0.001 downsamples 87 most-common words\n",
      "2017-05-14 17:15:01,243 : INFO : downsampling leaves estimated 41 word corpus (37.7% of prior 111)\n",
      "2017-05-14 17:15:01,245 : INFO : estimated required memory for 87 words and 100 dimensions: 125100 bytes\n",
      "2017-05-14 17:15:01,246 : INFO : resetting layer weights\n",
      "2017-05-14 17:15:01,251 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,261 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,265 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,269 : INFO : training on 555 raw words (313 effective words) took 0.0s, 35761 effective words/s\n",
      "2017-05-14 17:15:01,270 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,271 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,277 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,278 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,286 : INFO : training on 555 raw words (321 effective words) took 0.0s, 30972 effective words/s\n",
      "2017-05-14 17:15:01,287 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,288 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,294 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,301 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,305 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,306 : INFO : training on 555 raw words (287 effective words) took 0.0s, 21707 effective words/s\n",
      "2017-05-14 17:15:01,307 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,308 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,311 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,312 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,317 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,319 : INFO : training on 555 raw words (316 effective words) took 0.0s, 37776 effective words/s\n",
      "2017-05-14 17:15:01,320 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,321 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,324 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,325 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,327 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,331 : INFO : training on 555 raw words (296 effective words) took 0.0s, 39375 effective words/s\n",
      "2017-05-14 17:15:01,332 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,333 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,336 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,337 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,339 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,343 : INFO : training on 555 raw words (311 effective words) took 0.0s, 40835 effective words/s\n",
      "2017-05-14 17:15:01,344 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,346 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,351 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,352 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,353 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,358 : INFO : training on 555 raw words (319 effective words) took 0.0s, 38227 effective words/s\n",
      "2017-05-14 17:15:01,359 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,360 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,362 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,369 : INFO : training on 555 raw words (294 effective words) took 0.0s, 45166 effective words/s\n",
      "2017-05-14 17:15:01,370 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,370 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,376 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,378 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,383 : INFO : training on 555 raw words (315 effective words) took 0.0s, 51119 effective words/s\n",
      "2017-05-14 17:15:01,383 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,384 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,387 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,390 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,393 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,394 : INFO : training on 555 raw words (300 effective words) took 0.0s, 42342 effective words/s\n",
      "2017-05-14 17:15:01,395 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,396 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,400 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,402 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,406 : INFO : training on 555 raw words (311 effective words) took 0.0s, 47132 effective words/s\n",
      "2017-05-14 17:15:01,407 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,407 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,410 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,411 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,417 : INFO : training on 555 raw words (315 effective words) took 0.0s, 46040 effective words/s\n",
      "2017-05-14 17:15:01,417 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,418 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,422 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,424 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,430 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,430 : INFO : training on 555 raw words (317 effective words) took 0.0s, 38381 effective words/s\n",
      "2017-05-14 17:15:01,431 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,432 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,435 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,442 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,442 : INFO : training on 555 raw words (314 effective words) took 0.0s, 38202 effective words/s\n",
      "2017-05-14 17:15:01,443 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,444 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,448 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,449 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,451 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,457 : INFO : training on 555 raw words (309 effective words) took 0.0s, 30502 effective words/s\n",
      "2017-05-14 17:15:01,458 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,459 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,463 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,471 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,474 : INFO : training on 555 raw words (314 effective words) took 0.0s, 29043 effective words/s\n",
      "2017-05-14 17:15:01,475 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,477 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,482 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,483 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,491 : INFO : training on 555 raw words (316 effective words) took 0.0s, 31290 effective words/s\n",
      "2017-05-14 17:15:01,493 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,496 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,503 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,506 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,510 : INFO : training on 555 raw words (320 effective words) took 0.0s, 37366 effective words/s\n",
      "2017-05-14 17:15:01,511 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,512 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,515 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,516 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,518 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,522 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,524 : INFO : training on 555 raw words (322 effective words) took 0.0s, 35058 effective words/s\n",
      "2017-05-14 17:15:01,525 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,526 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,529 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,530 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,537 : INFO : training on 555 raw words (307 effective words) took 0.0s, 40199 effective words/s\n",
      "2017-05-14 17:15:01,537 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,538 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,541 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,545 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,548 : INFO : training on 555 raw words (312 effective words) took 0.0s, 41964 effective words/s\n",
      "2017-05-14 17:15:01,549 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,550 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,553 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,561 : INFO : training on 555 raw words (304 effective words) took 0.0s, 39411 effective words/s\n",
      "2017-05-14 17:15:01,562 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,563 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,567 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,573 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,574 : INFO : training on 555 raw words (308 effective words) took 0.0s, 41735 effective words/s\n",
      "2017-05-14 17:15:01,575 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,576 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,579 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,581 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,582 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,587 : INFO : training on 555 raw words (307 effective words) took 0.0s, 40713 effective words/s\n",
      "2017-05-14 17:15:01,588 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,588 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,592 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,600 : INFO : training on 555 raw words (309 effective words) took 0.0s, 35209 effective words/s\n",
      "2017-05-14 17:15:01,601 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,602 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,606 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,607 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,610 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,611 : INFO : training on 555 raw words (290 effective words) took 0.0s, 48756 effective words/s\n",
      "2017-05-14 17:15:01,612 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,613 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,615 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,616 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,617 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,622 : INFO : training on 555 raw words (313 effective words) took 0.0s, 44203 effective words/s\n",
      "2017-05-14 17:15:01,623 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,624 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,626 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,628 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,634 : INFO : training on 555 raw words (318 effective words) took 0.0s, 37212 effective words/s\n",
      "2017-05-14 17:15:01,635 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,636 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,640 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,646 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,646 : INFO : training on 555 raw words (316 effective words) took 0.0s, 44612 effective words/s\n",
      "2017-05-14 17:15:01,647 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,648 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,651 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,652 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,662 : INFO : training on 555 raw words (314 effective words) took 0.0s, 28581 effective words/s\n",
      "2017-05-14 17:15:01,662 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,663 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,666 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,667 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,674 : INFO : training on 555 raw words (313 effective words) took 0.0s, 38061 effective words/s\n",
      "2017-05-14 17:15:01,675 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,675 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,678 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,685 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,687 : INFO : training on 555 raw words (312 effective words) took 0.0s, 33076 effective words/s\n",
      "2017-05-14 17:15:01,689 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,689 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,695 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,696 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,698 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,705 : INFO : training on 555 raw words (313 effective words) took 0.0s, 30414 effective words/s\n",
      "2017-05-14 17:15:01,706 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,707 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,712 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,716 : INFO : training on 555 raw words (303 effective words) took 0.0s, 44010 effective words/s\n",
      "2017-05-14 17:15:01,717 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,719 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,727 : INFO : training on 555 raw words (307 effective words) took 0.0s, 50524 effective words/s\n",
      "2017-05-14 17:15:01,728 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,729 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,733 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,734 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,740 : INFO : training on 555 raw words (313 effective words) took 0.0s, 38246 effective words/s\n",
      "2017-05-14 17:15:01,741 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,743 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,749 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,750 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,758 : INFO : training on 555 raw words (302 effective words) took 0.0s, 33074 effective words/s\n",
      "2017-05-14 17:15:01,759 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,759 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,763 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,764 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,765 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,770 : INFO : training on 555 raw words (321 effective words) took 0.0s, 40457 effective words/s\n",
      "2017-05-14 17:15:01,771 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,772 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,775 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,776 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,783 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,783 : INFO : training on 555 raw words (315 effective words) took 0.0s, 35641 effective words/s\n",
      "2017-05-14 17:15:01,784 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,785 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,788 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,790 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,795 : INFO : training on 555 raw words (337 effective words) took 0.0s, 46720 effective words/s\n",
      "2017-05-14 17:15:01,796 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,797 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,800 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,808 : INFO : training on 555 raw words (286 effective words) took 0.0s, 32886 effective words/s\n",
      "2017-05-14 17:15:01,809 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,809 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,812 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,815 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,819 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,820 : INFO : training on 555 raw words (297 effective words) took 0.0s, 39499 effective words/s\n",
      "2017-05-14 17:15:01,820 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,821 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,824 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,825 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,832 : INFO : training on 555 raw words (294 effective words) took 0.0s, 37134 effective words/s\n",
      "2017-05-14 17:15:01,833 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,834 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,837 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,844 : INFO : training on 555 raw words (316 effective words) took 0.0s, 40455 effective words/s\n",
      "2017-05-14 17:15:01,845 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,846 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,849 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,850 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,851 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,856 : INFO : training on 555 raw words (301 effective words) took 0.0s, 40586 effective words/s\n",
      "2017-05-14 17:15:01,856 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,857 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,860 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,861 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,863 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,867 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,867 : INFO : training on 555 raw words (305 effective words) took 0.0s, 39847 effective words/s\n",
      "2017-05-14 17:15:01,868 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,869 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,872 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,872 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,873 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,877 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,878 : INFO : training on 555 raw words (300 effective words) took 0.0s, 44718 effective words/s\n",
      "2017-05-14 17:15:01,878 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,879 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,882 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,892 : INFO : training on 555 raw words (310 effective words) took 0.0s, 31302 effective words/s\n",
      "2017-05-14 17:15:01,892 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,893 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,897 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,899 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,900 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,901 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,901 : INFO : training on 555 raw words (310 effective words) took 0.0s, 58481 effective words/s\n",
      "2017-05-14 17:15:01,902 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,903 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,907 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,908 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,909 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,913 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,914 : INFO : training on 555 raw words (318 effective words) took 0.0s, 42203 effective words/s\n",
      "2017-05-14 17:15:01,914 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,915 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,918 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,919 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,921 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,925 : INFO : training on 555 raw words (314 effective words) took 0.0s, 44234 effective words/s\n",
      "2017-05-14 17:15:01,926 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,927 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,930 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,934 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,935 : INFO : training on 555 raw words (328 effective words) took 0.0s, 54453 effective words/s\n",
      "2017-05-14 17:15:01,936 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,937 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,940 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,941 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,946 : INFO : training on 555 raw words (323 effective words) took 0.0s, 50623 effective words/s\n",
      "2017-05-14 17:15:01,946 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,947 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,952 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,955 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,957 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,963 : INFO : training on 555 raw words (331 effective words) took 0.0s, 30466 effective words/s\n",
      "2017-05-14 17:15:01,964 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,965 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,971 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,978 : INFO : training on 555 raw words (302 effective words) took 0.0s, 39543 effective words/s\n",
      "2017-05-14 17:15:01,980 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,980 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,984 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:01,984 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:01,986 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:01,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:01,991 : INFO : training on 555 raw words (309 effective words) took 0.0s, 38791 effective words/s\n",
      "2017-05-14 17:15:01,992 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:01,993 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:01,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,012 : INFO : training on 555 raw words (311 effective words) took 0.0s, 24486 effective words/s\n",
      "2017-05-14 17:15:02,012 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,013 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,016 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,024 : INFO : training on 555 raw words (321 effective words) took 0.0s, 39916 effective words/s\n",
      "2017-05-14 17:15:02,024 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,025 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,029 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,030 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,037 : INFO : training on 555 raw words (311 effective words) took 0.0s, 37278 effective words/s\n",
      "2017-05-14 17:15:02,038 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,039 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,041 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,043 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,048 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,049 : INFO : training on 555 raw words (294 effective words) took 0.0s, 35904 effective words/s\n",
      "2017-05-14 17:15:02,050 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,052 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,055 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,055 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,062 : INFO : training on 555 raw words (299 effective words) took 0.0s, 39227 effective words/s\n",
      "2017-05-14 17:15:02,062 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,063 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,069 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,074 : INFO : training on 555 raw words (301 effective words) took 0.0s, 39607 effective words/s\n",
      "2017-05-14 17:15:02,075 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,076 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,081 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,082 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,086 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,090 : INFO : training on 555 raw words (318 effective words) took 0.0s, 34263 effective words/s\n",
      "2017-05-14 17:15:02,091 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,092 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,095 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,096 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,097 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,104 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,104 : INFO : training on 555 raw words (319 effective words) took 0.0s, 33147 effective words/s\n",
      "2017-05-14 17:15:02,105 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,106 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,109 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,110 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,112 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,117 : INFO : training on 555 raw words (330 effective words) took 0.0s, 40261 effective words/s\n",
      "2017-05-14 17:15:02,118 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,119 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,122 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,124 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,131 : INFO : training on 555 raw words (319 effective words) took 0.0s, 36635 effective words/s\n",
      "2017-05-14 17:15:02,132 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,134 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,139 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,142 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,143 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,144 : INFO : training on 555 raw words (302 effective words) took 0.0s, 47406 effective words/s\n",
      "2017-05-14 17:15:02,145 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,145 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,149 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,153 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,155 : INFO : training on 555 raw words (298 effective words) took 0.0s, 46477 effective words/s\n",
      "2017-05-14 17:15:02,156 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,157 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,160 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,164 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,165 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,165 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,166 : INFO : training on 555 raw words (312 effective words) took 0.0s, 47776 effective words/s\n",
      "2017-05-14 17:15:02,167 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,168 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,172 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,172 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,176 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,178 : INFO : training on 555 raw words (306 effective words) took 0.0s, 47740 effective words/s\n",
      "2017-05-14 17:15:02,179 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,180 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,183 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,187 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,191 : INFO : training on 555 raw words (305 effective words) took 0.0s, 38166 effective words/s\n",
      "2017-05-14 17:15:02,192 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,193 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,198 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,200 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,201 : INFO : training on 555 raw words (316 effective words) took 0.0s, 97783 effective words/s\n",
      "2017-05-14 17:15:02,203 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,203 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,207 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,211 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,213 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,214 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,215 : INFO : training on 555 raw words (316 effective words) took 0.0s, 38945 effective words/s\n",
      "2017-05-14 17:15:02,215 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,216 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,222 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,227 : INFO : training on 555 raw words (311 effective words) took 0.0s, 43957 effective words/s\n",
      "2017-05-14 17:15:02,228 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,230 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,232 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,237 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,239 : INFO : training on 555 raw words (305 effective words) took 0.0s, 42262 effective words/s\n",
      "2017-05-14 17:15:02,240 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,241 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,250 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,258 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,259 : INFO : training on 555 raw words (299 effective words) took 0.0s, 28667 effective words/s\n",
      "2017-05-14 17:15:02,261 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,262 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,265 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,271 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,272 : INFO : training on 555 raw words (309 effective words) took 0.0s, 43638 effective words/s\n",
      "2017-05-14 17:15:02,273 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,274 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,279 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,280 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,283 : INFO : training on 555 raw words (316 effective words) took 0.0s, 77151 effective words/s\n",
      "2017-05-14 17:15:02,284 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,285 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,288 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,295 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,295 : INFO : training on 555 raw words (320 effective words) took 0.0s, 44868 effective words/s\n",
      "2017-05-14 17:15:02,296 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,297 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,301 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,306 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,310 : INFO : training on 555 raw words (293 effective words) took 0.0s, 31160 effective words/s\n",
      "2017-05-14 17:15:02,311 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,312 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,314 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,319 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,320 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,321 : INFO : training on 555 raw words (310 effective words) took 0.0s, 42757 effective words/s\n",
      "2017-05-14 17:15:02,322 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,323 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,326 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,331 : INFO : training on 555 raw words (308 effective words) took 0.0s, 53006 effective words/s\n",
      "2017-05-14 17:15:02,332 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,333 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,336 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,339 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,341 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,342 : INFO : training on 555 raw words (304 effective words) took 0.0s, 50378 effective words/s\n",
      "2017-05-14 17:15:02,343 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,344 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,347 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,350 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,351 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,353 : INFO : training on 555 raw words (303 effective words) took 0.0s, 50596 effective words/s\n",
      "2017-05-14 17:15:02,354 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,355 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,359 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,361 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,363 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,363 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,364 : INFO : training on 555 raw words (310 effective words) took 0.0s, 53792 effective words/s\n",
      "2017-05-14 17:15:02,365 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,365 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,368 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,371 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,373 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,375 : INFO : training on 555 raw words (297 effective words) took 0.0s, 45036 effective words/s\n",
      "2017-05-14 17:15:02,375 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,376 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,378 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,381 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,384 : INFO : training on 555 raw words (315 effective words) took 0.0s, 55677 effective words/s\n",
      "2017-05-14 17:15:02,385 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,386 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,388 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,391 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,392 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,393 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,394 : INFO : training on 555 raw words (321 effective words) took 0.0s, 54802 effective words/s\n",
      "2017-05-14 17:15:02,395 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,396 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,400 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,402 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,404 : INFO : training on 555 raw words (309 effective words) took 0.0s, 55064 effective words/s\n",
      "2017-05-14 17:15:02,404 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,405 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,407 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,410 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,411 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,412 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,413 : INFO : training on 555 raw words (320 effective words) took 0.0s, 60656 effective words/s\n",
      "2017-05-14 17:15:02,414 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,414 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,417 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,419 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,423 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,423 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,424 : INFO : training on 555 raw words (302 effective words) took 0.0s, 41165 effective words/s\n",
      "2017-05-14 17:15:02,425 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,425 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,428 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,432 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,434 : INFO : training on 555 raw words (310 effective words) took 0.0s, 54550 effective words/s\n",
      "2017-05-14 17:15:02,435 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,435 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,440 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,443 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,445 : INFO : training on 555 raw words (293 effective words) took 0.0s, 51348 effective words/s\n",
      "2017-05-14 17:15:02,446 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,447 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,449 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,450 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,453 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,455 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,455 : INFO : training on 555 raw words (288 effective words) took 0.0s, 44376 effective words/s\n",
      "2017-05-14 17:15:02,456 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,457 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,460 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,461 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,465 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,466 : INFO : training on 555 raw words (301 effective words) took 0.0s, 54927 effective words/s\n",
      "2017-05-14 17:15:02,466 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,467 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,473 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,477 : INFO : training on 555 raw words (316 effective words) took 0.0s, 45118 effective words/s\n",
      "2017-05-14 17:15:02,478 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,478 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,483 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,484 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,486 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,488 : INFO : training on 555 raw words (304 effective words) took 0.0s, 43359 effective words/s\n",
      "2017-05-14 17:15:02,489 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,490 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,496 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,505 : INFO : training on 555 raw words (299 effective words) took 0.0s, 30119 effective words/s\n",
      "2017-05-14 17:15:02,505 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,506 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,513 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,516 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,518 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,520 : INFO : training on 555 raw words (304 effective words) took 0.0s, 44309 effective words/s\n",
      "2017-05-14 17:15:02,520 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,521 : INFO : training model with 4 workers on 87 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-14 17:15:02,525 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-14 17:15:02,528 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-14 17:15:02,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-14 17:15:02,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-14 17:15:02,530 : INFO : training on 555 raw words (307 effective words) took 0.0s, 58498 effective words/s\n",
      "2017-05-14 17:15:02,531 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-14 17:15:02,532 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9999162   0.98256967  0.96522222  0.63741083  0.41868507  0.97363492\n",
      "   0.71758412  0.99872238  0.99668273  0.97257676]\n",
      " [ 0.99566793  0.86712646  0.93974316  0.          0.          0.85569304\n",
      "   0.          0.93151903  0.93891066  0.84126103]\n",
      " [ 0.99566793  0.86712646  0.9397431   0.          0.          0.85569304\n",
      "   0.52657533  0.93151903  0.9389106   0.84126103]\n",
      " [ 1.          1.          1.          0.          0.          1.          0.\n",
      "   1.          1.          1.        ]\n",
      " [ 1.          1.          1.          0.          0.          1.          1.\n",
      "   1.          1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(doc2vecs_features(sentences_train[:10,:]))\n",
    "# sentences = []\n",
    "# stemmer = PorterStemmer()\n",
    "# for i in range(10): \n",
    "#     source_sentence = sentences_train[i,0].lower().split(\" \") \n",
    "#     source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "#     unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "#     sentences.append(unigrams_que1)\n",
    "        \n",
    "#     target_sentence = sentences_train[i,1].lower().split(\" \") \n",
    "#     target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "#     unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "#     sentences.append(unigrams_que2)\n",
    "\n",
    "# texts=sentences.copy()\n",
    "# documents = []\n",
    "# ct = 0\n",
    "# for doc in texts:\n",
    "#     doc = gensim.models.doc2vec.LabeledSentence(words = doc, tags = ['SENT_'+str(ct)])\n",
    "#     ct+=1\n",
    "#     documents.append(doc)\n",
    "# model = gensim.models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1,workers=4)\n",
    "# model.build_vocab(documents)\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     model.train(documents,total_examples=model.corpus_count,epochs=model.iter)\n",
    "#     #model.alpha -= 0.002  # decrease the learning rate`\n",
    "#     #model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "# #model.save(\"my_model.doc2vec\")\n",
    "# #model_loaded = gensim.models.Doc2Vec.load('my_model.doc2vec')\n",
    "\n",
    "# print (model.docvecs.most_similar([\"SENT_5\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency and Hash Tag ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#frquency questions and hash.\n",
    "train_orig =  pd.read_csv('train.csv', header=0)\n",
    "test_orig =  pd.read_csv('test.csv', header=0)\n",
    "\n",
    "tic0=timeit.default_timer()\n",
    "df1 = train_orig[['question1']].copy()\n",
    "df2 = train_orig[['question2']].copy()\n",
    "df1_test = test_orig[['question1']].copy()\n",
    "df2_test = test_orig[['question2']].copy()\n",
    "\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions = train_questions.append(df1_test)\n",
    "train_questions = train_questions.append(df2_test)\n",
    "#train_questions.drop_duplicates(subset = ['qid1'],inplace=True)\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "train_cp = train_orig.copy()\n",
    "test_cp = test_orig.copy()\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "test_cp['is_duplicate'] = -1\n",
    "test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_features(data_set,nb_ex):\n",
    "    # Select basic features\n",
    "    tfidf_cosin_sim = []     \n",
    "    stemmer = PorterStemmer()\n",
    "    dif_len = []\n",
    "    common_unigrams_lens = []\n",
    "    common_unigrams_ratios = []\n",
    "    common_bigrams_lens = []\n",
    "    common_bigrams_ratios = []\n",
    "    common_trigrams_lens = []\n",
    "    common_trigrams_ratios = []\n",
    "    similarities = []\n",
    "    counter = 0\n",
    "    sentences = []\n",
    "    for i in range(nb_ex):\n",
    "        if i%10000== 0:\n",
    "            print(i)\n",
    "        # select informations about articles 1 and 2\n",
    "        \n",
    "        \n",
    "        \n",
    "        source_sentence = data_set[i,0].lower().split(\" \") \n",
    "        source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "        unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "        sentences.append(unigrams_que1)\n",
    "        \n",
    "        target_sentence = data_set[i,1].lower().split(\" \") \n",
    "        target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "        unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "        sentences.append(unigrams_que2)\n",
    "\n",
    "        \n",
    "        #get unigram features #\n",
    "        common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
    "        common_unigrams_lens.append(common_unigrams_len)\n",
    "        common_unigrams_ratios.append(float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1))\n",
    "        \n",
    "        # get bigram features #\n",
    "        bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n",
    "        bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n",
    "        common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n",
    "        common_bigrams_lens.append(common_bigrams_len)\n",
    "        common_bigrams_ratios.append(float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1))\n",
    "\n",
    "\n",
    "        # get trigram features #\n",
    "        trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n",
    "        trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n",
    "        common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n",
    "        common_trigrams_lens.append(common_trigrams_len)\n",
    "        common_trigrams_ratios.append(float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1))\n",
    "\n",
    "        dif_len.append(abs(len(source_sentence) - len(target_sentence)))\n",
    "        \n",
    "    \n",
    "    \n",
    "#     #Cosine Similarity on TF IDF vectors\n",
    "#     texts = sentences\n",
    "\n",
    "#     tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, #max_features=n_features,\n",
    "#                                    stop_words='english')\n",
    "#     tfidf_sentences= tfidf_vectorizer.fit_transform(data_set[:nb_ex,:].flatten())\n",
    "    \n",
    "#     for i in range(nb_ex):\n",
    "#         tfidf_cosin_sim.append(cosine_similarity(tfidf_sentences[2*i,:], tfidf_sentences[2*i+1,:])[0,0])\n",
    "  \n",
    "    \n",
    "    \n",
    "# #     #LSI similarities\n",
    "\n",
    "#     dictionary = corpora.Dictionary(texts)\n",
    "#     corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#     lsi = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=100)\n",
    "#     index = gensim.similarities.MatrixSimilarity(lsi[corpus])\n",
    "#     for i in range(nb_ex):\n",
    "#         similarities.append(index[lsi[corpus[2*i]]][2*i+1])\n",
    "    \n",
    "\n",
    "    # examples as rows, features as columns\n",
    "    features = np.array([#tfidf_cosin_sim,\n",
    "                         #similarities,\n",
    "                         common_unigrams_lens,\n",
    "                         common_unigrams_ratios,\n",
    "                         common_bigrams_lens,\n",
    "                         common_bigrams_ratios,\n",
    "                         common_trigrams_lens,\n",
    "                         common_trigrams_ratios,\n",
    "                         dif_len]).T\n",
    "    \n",
    "    \n",
    "    #normalized_features = preprocessing.scale(features) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_ex = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "n_grams_features,y= select_features(sentences_train,nb_ex),labels[:nb_ex].astype(float)#,lsi,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 7)\n"
     ]
    }
   ],
   "source": [
    "print(n_grams_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features = train_comb[['q1_hash','q2_hash','q1_freq','q2_freq']].iloc[:nb_ex].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(new_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.column_stack([n_grams_features,new_features])\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.393326124948\n",
      "Testing Score: 0.395367589338\n"
     ]
    }
   ],
   "source": [
    "#clf = LogisticRegression()\n",
    "clf = RandomForestClassifier(n_estimators=50,max_depth = 5,max_features=4)\n",
    "clf.fit(X_train,y_train)\n",
    "print('Training Score:',sklearn.metrics.log_loss(y_train,clf.predict_proba(X_train)[:,1]))\n",
    "print('Testing Score:',sklearn.metrics.log_loss(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features=4, max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "Testing_Set = pd.read_csv('test.csv')\n",
    "print(Testing_Set.shape)\n",
    "clean_data_test = Testing_Set[['question1','question2']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_basic_features_test(data_set):\n",
    "    # Select basic features\n",
    "    tfidf_cosin_sim = []     \n",
    "    stemmer = PorterStemmer()\n",
    "    dif_len = []\n",
    "    common_unigrams_lens = []\n",
    "    common_unigrams_ratios = []\n",
    "    common_bigrams_lens = []\n",
    "    common_bigrams_ratios = []\n",
    "    common_trigrams_lens = []\n",
    "    common_trigrams_ratios = []\n",
    "    similarities = []\n",
    "    counter = 0\n",
    "    sentences = []\n",
    "    for i in range(data_set.shape[0]):\n",
    "        if i%10000== 0:\n",
    "            print(i)\n",
    "        # select informations about articles 1 and 2\n",
    "        \n",
    "        \n",
    "        \n",
    "        source_sentence = str(data_set[i,0]).lower().split(\" \") \n",
    "        source_sentence = [token for token in source_sentence if token not in stpwds]\n",
    "        unigrams_que1 = [stemmer.stem(token) for token in source_sentence]\n",
    "        sentences.append(unigrams_que1)\n",
    "        \n",
    "        target_sentence = str(data_set[i,1]).lower().split(\" \") \n",
    "        target_sentence= [token for token in target_sentence if token not in stpwds]\n",
    "        unigrams_que2 = [stemmer.stem(token) for token in target_sentence]\n",
    "        sentences.append(unigrams_que2)\n",
    "\n",
    "        \n",
    "        #get unigram features #\n",
    "        common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
    "        common_unigrams_lens.append(common_unigrams_len)\n",
    "        common_unigrams_ratios.append(float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1))\n",
    "        \n",
    "        # get bigram features #\n",
    "        bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n",
    "        bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n",
    "        common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n",
    "        common_bigrams_lens.append(common_bigrams_len)\n",
    "        common_bigrams_ratios.append(float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1))\n",
    "\n",
    "\n",
    "        # get trigram features #\n",
    "        trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n",
    "        trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n",
    "        common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n",
    "        common_trigrams_lens.append(common_trigrams_len)\n",
    "        common_trigrams_ratios.append(float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1))\n",
    "\n",
    "        dif_len.append(abs(len(source_sentence) - len(target_sentence)))\n",
    "        \n",
    "    \n",
    "    \n",
    "#     # Cosine Similarity on TF IDF vectors\n",
    "    texts = sentences\n",
    "\n",
    "#     tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, #max_features=n_features,\n",
    "#                                    stop_words='english')\n",
    "#     tfidf_sentences= tfidf_vectorizer.fit_transform(data_set.flatten())\n",
    "    \n",
    "#     for i in range(data_set.shape[0]):\n",
    "#         tfidf_cosin_sim.append(cosine_similarity(tfidf_sentences[2*i,:], tfidf_sentences[2*i+1,:])[0,0])\n",
    "  \n",
    "    \n",
    "    \n",
    "    #LSI similarities\n",
    "\n",
    "    #dictionary = corpora.Dictionary(texts)\n",
    "    #corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #for i in range(data_set.shape[0]):\n",
    "    #    similarities.append(index[lsi[corpus[2*i]]][2*i+1])\n",
    "    \n",
    "\n",
    "    # examples as rows, features as columns\n",
    "    features = np.array([#tfidf_cosin_sim,\n",
    "                         #similarities,\n",
    "                         common_unigrams_lens,\n",
    "                         common_unigrams_ratios,\n",
    "                         common_bigrams_lens,\n",
    "                         common_bigrams_ratios,\n",
    "                         common_trigrams_lens,\n",
    "                         common_trigrams_ratios,\n",
    "                         dif_len]).T\n",
    "    \n",
    "    \n",
    "    # scale\n",
    "    #normalized_features = preprocessing.scale(features) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic_features = select_basic_features_test(clean_data_test)\n",
    "new_features_test = test_comb[['q1_hash','q2_hash','q1_freq','q2_freq']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(basic_features.shape,new_features_test.shape)\n",
    "X_submission = np.column_stack([basic_features,new_features_test])\n",
    "X_submission = preprocessing.scale(X_submission)\n",
    "y_submission = clf.predict_proba(X_submission)[:,1]\n",
    "Testing_Set['is_duplicate'] = y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = Testing_Set[['test_id','is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = submission.to_csv('/Users/pascalsitbon/work/Kaggle/pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
